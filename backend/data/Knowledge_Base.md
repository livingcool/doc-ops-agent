# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.

## Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

## How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

--- Snippet 1 (Source: data\Knowledge_Base.md) ---
2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

--- Snippet 2 (Source: agent_logic.py) ---
# --- Step 9: Log the final result ---
        if "Successfully" in result_message:
            # On success, log the specific format you requested.
            log_entry = (
                f"This is an AI-generated documentation update for PR #{pr_number}, "
                f"originally authored by @{user_name}.\n"
                f"Original PR: '{pr_title}' AI Analysis: {analysis_summary}"
            )
            logger.info(log_entry)
        else:
            # On failure, log a simpler error message for clarity.
            logger.error(
                f"AGENT FAILED for PR #{pr_number} ({repo_name}). Reason: {result_message}"
            )

    except Exception as e:
        logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
        await broadcaster("log-error", f"Agent failed with error: {e}")
        return

--- Snippet 3 (Source: agent_logic.py) ---
await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
        
        # --- THIS IS THE CORE LOGIC CHANGE ---
        if not retrieved_docs:
            # --- CREATE MODE ---
            await broadcaster("log-step", "No relevant docs found. Switching to 'Create Mode'...")
            new_documentation = await creator_chain.ainvoke({
                "analysis_summary": analysis_summary,
                "git_diff": git_diff
            })
            # For creation, the source file is always the main knowledge base
            raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
        else:
            # --- UPDATE MODE ---
            # Make the confidence threshold configurable for easier testing
            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                return

--- Snippet 4 (Source: agent_logic.py) ---
def _append_to_file_sync(file_path: str, content: str):
    """Synchronous file append operation."""
    with open(file_path, "a", encoding="utf-8") as f:
        f.write(content)

# --- Updated Core Agent Logic ---

async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str, repo_name: str, pr_number: str, user_name: str):
    """This is the main 'brain' of the agent. It runs the full analysis-retrieval-rewrite pipeline."""
    
    if not retriever:
        print("Agent failed: AI components are not initialized.")
        await broadcaster("log-error", "Error: Agent AI components are not ready.")
        return

--- Snippet 5 (Source: agent_logic.py) ---
if "Error" in pr_url:
                result_message = f"Failed to create PR. Reason: {pr_url}"
                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
            else:
                result_message = f"Successfully created documentation PR: {pr_url}"
                await broadcaster("log-action", f"✅ Successfully created PR: {pr_url}")

        except Exception as e:
            result_message = f"Agent failed during PR creation with error: {e}"
            await broadcaster("log-error", f"Agent failed with error: {e}")
            # Log the exception traceback for debugging
            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
        
---
### Relevant Code Changes
```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
 
     # Your OpenAI API key
     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
+
+    # (Optional) The minimum confidence score required to update a document
+    CONFIDENCE_THRESHOLD=0.2
     ```
 
 ### Step 3: Frontend Setup
@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
     *   **Build Command**: `pip install -r requirements.txt`
         *   This is usually the default and is correct.
     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-        *   Use `10000` for the port as recommended by Render.
+        *   Use the port recommended by Render (e.g., `10000`).
 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
 4.  **Deploy**: Trigger a manual deploy.
 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
         else:
             # --- UPDATE MODE ---
-            if confidence_score < 0.2: # Gatekeeping based on confidence
+            # Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
```


---

### AI-Generated Update (2025-11-16 11:50:12)

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# --- Step 9: Log the final result ---
        if "Successfully" in result_message:
            # On success, log the specific format you requested.
            log_entry = (
                f"This is an AI-generated documentation update for PR #{pr_number}, "
                f"originally authored by @{user_name}.\n"
                f"Original PR: '{pr_title}' AI Analysis: {analysis_summary}"
            )
            logger.info(log_entry)
        else:
            # On failure, log a simpler error message for clarity.
            logger.error(
                f"AGENT FAILED for PR #{pr_number} ({repo_name}). Reason: {result_message}"
            )

    except Exception as e:
        logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
        await broadcaster("log-error", f"Agent failed with error: {e}")
        return

---

await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
        
        # --- THIS IS THE CORE LOGIC CHANGE ---
        if not retrieved_docs:
            # --- CREATE MODE ---
            await broadcaster("log-step", "No relevant docs found. Switching to 'Create Mode'...")
            new_documentation = await creator_chain.ainvoke({
                "analysis_summary": analysis_summary,
                "git_diff": git_diff
            })
            # For creation, the source file is always the main knowledge base
            raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
        else:
            # --- UPDATE MODE ---
            # Make the confidence threshold configurable for easier testing
            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                return

---

def _append_to_file_sync(file_path: str, content: str):
    """Synchronous file append operation."""
    with open(file_path, "a", encoding="utf-8") as f:
        f.write(content)

# --- Updated Core Agent Logic ---

async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str, repo_name: str, pr_number: str, user_name: str):
    """This is the main 'brain' of the agent. It runs the full analysis-retrieval-rewrite pipeline."""
    
    if not retriever:
        print("Agent failed: AI components are not initialized.")
        await broadcaster("log-error", "Error: Agent AI components are not ready.")
        return

---

if "Error" in pr_url:
                result_message = f"Failed to create PR. Reason: {pr_url}"
                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
            else:
                result_message = f"Successfully created documentation PR: {pr_url}"
                await broadcaster("log-action", f"✅ Successfully created PR: {pr_url}")

        except Exception as e:
            result_message = f"Agent failed during PR creation with error: {e}"
            await broadcaster("log-error", f"Agent failed with error: {e}")
            # Log the exception traceback for debugging
            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
        
---
### Relevant Code Changes
```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
 
     # Your OpenAI API key
     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
+
+    # (Optional) The minimum confidence score required to update a document
+    CONFIDENCE_THRESHOLD=0.2
     ```
 
 ### Step 3: Frontend Setup
@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
     *   **Build Command**: `pip install -r requirements.txt`
         *   This is usually the default and is correct.
     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-        *   Use `10000` for the port as recommended by Render.
+        *   Use the port recommended by Render (e.g., `10000`).
 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
 4.  **Deploy**: Trigger a manual deploy.
 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
         else:
             # --- UPDATE MODE ---
-            if confidence_score < 0.2: # Gatekeeping based on confidence
+            # Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
```


---

### AI-Generated Update (2025-11-16 11:50:33)

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

---
### Relevant Code Changes
```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
 
     # Your OpenAI API key
     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
+
+    # (Optional) The minimum confidence score required to update a document
+    CONFIDENCE_THRESHOLD=0.2
     ```
 
 ### Step 3: Frontend Setup
@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
     *   **Build Command**: `pip install -r requirements.txt`
         *   This is usually the default and is correct.
     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-        *   Use `10000` for the port as recommended by Render.
+        *   Use the port recommended by Render (e.g., `10000`).
 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
 4.  **Deploy**: Trigger a manual deploy.
 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
         else:
             # --- UPDATE MODE ---
-            if confidence_score < 0.2: # Gatekeeping based on confidence
+            # Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
```


---

### AI-Generated Update (2025-11-16 11:50:38)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---
### Relevant Code Changes
```diff
diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
index 36e9b76..2801bec 100644
--- a/backend/data/Knowledge_Base.md
+++ b/backend/data/Knowledge_Base.md
@@ -6,65 +6,13 @@ This project provides an automated agent that monitors GitHub repositories for c
 
 The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
 
-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
-
-## Core Technologies
-
-*   **Python:** The primary programming language for the backend application.
-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
-*   **python-dotenv:** Used to load environment variables from a `.env` file.
-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
-
-## How Components Work Together
-
-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
-
-1.  **GitHub Webhook (`/api/webhook/github`):**
-    *   This endpoint receives `POST` requests from GitHub.
-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
-
- 2.  **Agent Logic (`agent_logic.py`):**
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
-
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+
+## Core Technologies
+
+*   **Python:** The primary programming language for the backend application.
+*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+*   **python-dotenv:** Used to load environment variables from a `.env` file.
+*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+
+## How Components Work Together
+
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
 
 ---
 
@@ -78,7 +26,8 @@ In essence, the system acts as an automated documentation assistant. It watches
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+---
+
 2.  **Agent Logic (`agent_logic.py`):**
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
@@ -87,7 +36,8 @@ In essence, the system acts as an automated documentation assistant. It watches
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
+---
+
 # --- Step 9: Log the final result ---
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
@@ -108,7 +58,8 @@ await broadcaster("log-error", f"Agent failed with error: {e}")
         return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
         
         # --- THIS IS THE CORE LOGIC CHANGE ---
@@ -129,7 +80,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippet
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -145,7 +97,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 5 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -200,4 +153,4 @@ index 194eafb..3b3f53d 100644
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
  
-```
+```
\ No newline at end of file
```


---

### AI-Generated Update (2025-11-16 11:50:55)

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.

## Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

## How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to determine the minimum confidence score required to proceed with updating existing documentation.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes
```diff
diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
index 36e9b76..a073f58 100644
--- a/backend/data/Knowledge_Base.md
+++ b/backend/data/Knowledge_Base.md
@@ -6,65 +6,53 @@ This project provides an automated agent that monitors GitHub repositories for c
 
 The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
 
-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
-
-## Core Technologies
-
-*   **Python:** The primary programming language for the backend application.
-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
-*   **python-dotenv:** Used to load environment variables from a `.env` file.
-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
-
-## How Components Work Together
-
-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
-
-1.  **GitHub Webhook (`/api/webhook/github`):**
-    *   This endpoint receives `POST` requests from GitHub.
-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
 
 2.  **Agent Logic (`agent_logic.py`):**
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
 
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+---
+
+# Doc-Ops Agent
+
+This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.
+
+## Purpose
+
+The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+---
+
 ---
 ### Relevant Code Changes
 ```diff
@@ -200,4 +119,4 @@ index 194eafb..3b3f53d 100644
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
  
-```
+```
\ No newline at end of file
```


---

### AI-Generated Update (2025-11-16 11:50:57)

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.

## Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

## How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes
```diff
diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
index 36e9b76..a073f58 100644
--- a/backend/data/Knowledge_Base.md
+++ b/backend/data/Knowledge_Base.md
@@ -6,65 +6,53 @@ This project provides an automated agent that monitors GitHub repositories for c
 
 The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
 
-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
-
-## Core Technologies
-
-*   **Python:** The primary programming language for the backend application.
-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
-*   **python-dotenv:** Used to load environment variables from a `.env` file.
-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
-
-## How Components Work Together
-
-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
-
-1.  **GitHub Webhook (`/api/webhook/github`):**
-    *   This endpoint receives `POST` requests from GitHub.
-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
 
 2.  **Agent Logic (`agent_logic.py`):**
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
 
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+---
+
+# Doc-Ops Agent
+
+This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.
+
+## Purpose
+
+The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+---
+
 ---
 ### Relevant Code Changes
 ```diff
@@ -200,4 +119,4 @@ index 194eafb..3b3f53d 100644
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
  
-```
+```
\ No newline at end of file
```


---

### AI-Generated Update (2025-11-16 11:51:00)

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.

## Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

## How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes
```diff
diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
index 36e9b76..2801bec 100644
--- a/backend/data/Knowledge_Base.md
+++ b/backend/data/Knowledge_Base.md
@@ -1,75 +1,3 @@
-# Doc-Ops Agent
-
-This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.
-
-## Purpose
-
-The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
-
-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
-
-## Core Technologies
-
-*   **Python:** The primary programming language for the backend application.
-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
-*   **python-dotenv:** Used to load environment variables from a `.env` file.
-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
-
-## How Components Work Together
-
-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
-
-1.  **GitHub Webhook (`/api/webhook/github`):**
-    *   This endpoint receives `POST` requests from GitHub.
-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
-
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
-
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+---
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
+
+# --- Step 9: Log the final result ---
+        if "Successfully" in result_message:
+            # On success, log the specific format you requested.
+            log_entry = (
+                f"This is an AI-generated documentation update for PR #{pr_number}, "
+                f"originally authored by @{user_name}.\n"
+                f"Original PR: '{pr_title}' AI Analysis: {analysis_summary}"
+            )
+            logger.info(log_entry)
+        else:
+            # On failure, log a simpler error message for clarity.
+            logger.error(
+                f"AGENT FAILED for PR #{pr_number} ({repo_name}). Reason: {result_message}"
+            )
+
+    except Exception as e:
+        logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+        await broadcaster("log-error", f"Agent failed with error: {e}")
+        return
+---
+
+await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+        
+        # --- THIS IS THE CORE LOGIC CHANGE ---
+        if not retrieved_docs:
+            # --- CREATE MODE ---
+            await broadcaster("log-step", "No relevant docs found. Switching to 'Create Mode'...")
+            new_documentation = await creator_chain.ainvoke({
+                "analysis_summary": analysis_summary,
+                "git_diff": git_diff
+            })
+            # For creation, the source file is always the main knowledge base
+            raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+        else:
+            # --- UPDATE MODE ---
+            # Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+---
+
+def _append_to_file_sync(file_path: str, content: str):
+    """Synchronous file append operation."""
+    with open(file_path, "a", encoding="utf-8") as f:
+        f.write(content)
+
+# --- Updated Core Agent Logic ---
+
+async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str, repo_name: str, pr_number: str, user_name: str):
+    """This is the main 'brain' of the agent. It runs the full analysis-retrieval-rewrite pipeline."""
+    
+    if not retriever:
+        print("Agent failed: AI components are not initialized.")
+        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+        return
+---
+
+if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+            else:
+                result_message = f"Successfully created documentation PR: {pr_url}"
+                await broadcaster("log-action", f"✅ Successfully created PR: {pr_url}")
+
+        except Exception as e:
+            result_message = f"Agent failed during PR creation with error: {e}"
+            await broadcaster("log-error", f"Agent failed with error: {e}")
+            # Log the exception traceback for debugging
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+
+```


---

### AI-Generated Update (2025-11-16 11:51:06)

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.

## Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

## How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes
```diff
diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
index 36e9b76..2801bec 100644
--- a/backend/data/Knowledge_Base.md
+++ b/backend/data/Knowledge_Base.md
@@ -1,75 +1,3 @@
-# Doc-Ops Agent
-
-This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.
-
-## Purpose
-
-The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
-
-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
-
-## Core Technologies
-
-*   **Python:** The primary programming language for the backend application.
-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
-*   **python-dotenv:** Used to load environment variables from a `.env` file.
-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
-
-## How Components Work Together
-
-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
-
-1.  **GitHub Webhook (`/api/webhook/github`):**
-    *   This endpoint receives `POST` requests from GitHub.
-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
-
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
-
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+
+## Core Technologies
+
+*   **Python:** The primary programming language for the backend application.
+*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+*   **python-dotenv:** Used to load environment variables from a `.env` file.
+*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+
+## How Components Work Together
+
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+## Agent Logic (`agent_logic.py`)
+
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+# Step 9: Log the final result
+        if "Successfully" in result_message:
+            # On success, log the specific format you requested.
+            log_entry = (
+                f"This is an AI-generated documentation update for PR #{pr_number}, "
+                f"originally authored by @{user_name}.\n"
+                f"Original PR: '{pr_title}' AI Analysis: {analysis_summary}"
+            )
+            logger.info(log_entry)
+        else:
+            # On failure, log a simpler error message for clarity.
+            logger.error(
+                f"AGENT FAILED for PR #{pr_number} ({repo_name}). Reason: {result_message}"
+            )
+
+    except Exception as e:
+        logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+        await broadcaster("log-error", f"Agent failed with error: {e}")
+        return
+
+---
+
+await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+        
+        # --- THIS IS THE CORE LOGIC CHANGE ---
+        if not retrieved_docs:
+            # --- CREATE MODE ---
+            await broadcaster("log-step", "No relevant docs found. Switching to 'Create Mode'...")
+            new_documentation = await creator_chain.ainvoke({
+                "analysis_summary": analysis_summary,
+                "git_diff": git_diff
+            })
+            # For creation, the source file is always the main knowledge base
+            raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+        else:
+            # --- UPDATE MODE ---
+            # Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+---
+
+def _append_to_file_sync(file_path: str, content: str):
+    """Synchronous file append operation."""
+    with open(file_path, "a", encoding="utf-8") as f:
+        f.write(content)
+
+# --- Updated Core Agent Logic ---
+
+async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str, repo_name: str, pr_number: str, user_name: str):
+    """This is the main 'brain' of the agent. It runs the full analysis-retrieval-rewrite pipeline."""
+    
+    if not retriever:
+        print("Agent failed: AI components are not initialized.")
+        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+        return
+
+---
+
+if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+            else:
+                result_message = f"Successfully created documentation PR: {pr_url}"
+                await broadcaster("log-action", f"✅ Successfully created PR: {pr_url}")
+
+        except Exception as e:
+            result_message = f"Agent failed during PR creation with error: {e}"
+            await broadcaster("log-error", f"Agent failed with error: {e}")
+            # Log the exception traceback for debugging
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+        
+```


---

### AI-Generated Update (2025-11-16 11:51:20)

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.

## Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

## How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes
```diff
diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
index 36e9b76..a073f58 100644
--- a/backend/data/Knowledge_Base.md
+++ b/backend/data/Knowledge_Base.md
@@ -6,65 +6,53 @@ This project provides an automated agent that monitors GitHub repositories for c
  
 -*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
 -*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
--*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
--*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
  
 -## Core Technologies
 -
 -*   **Python:** The primary programming language for the backend application.
--*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
--*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
--*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
--*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
--*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
--*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
--*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
--*   **python-dotenv:** Used to load environment variables from a `.env` file.
--*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
  
 -## How Components Work Together
 -
 -The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
  
 -1.  **GitHub Webhook (`/api/webhook/github`):**
 -    *   This endpoint receives `POST` requests from GitHub.
 -    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
 -    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
 -    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
 -    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
 -    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
  
 -2.  **Agent Logic (`agent_logic.py`):**
 -    *   **`run_agent_analysis`:** This is the core of the agent.
 -        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
 -        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
 -        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
 -        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
 -        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 -        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
 -        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
 -3.  **Vector Store (`vector_store.py`):**
 -    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
 -        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
 -        *   It loads all `.md` files from the `data/` directory.
 -        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
 -        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
 -        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
 -    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
 -    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+-3.  **Vector Store (`vector_store.py`):**
+-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+-        *   It loads all `.md` files from the `data/` directory.
+-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
  
 -4.  **Live Logging (`/api/stream/logs`):**
 -    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
 -    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
 -
 -5.  **Environment Variables:**
 -    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
 -    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+-    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
  
 -In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
-+
- ---
- ### Relevant Code Changes
- ```diff
-@@ -200,4 +119,4 @@ index 194eafb..3b3f53d 100644
-                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                  return
-  
--```
-+```
-\ No newline at end of file
-```
-\ No newline at end of file
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
 ```


---

### AI-Generated Update (2025-11-16 11:51:30)

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.

## Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

## How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

## Agent Logic (`agent_logic.py`)

*   **`run_agent_analysis`:** This is the core of the agent.
    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Step 9: Log the final result
        if "Successfully" in result_message:
            # On success, log the specific format you requested.
            log_entry = (
                f"This is an AI-generated documentation update for PR #{pr_number}, "
                f"originally authored by @{user_name}.\n"
                f"Original PR: '{pr_title}' AI Analysis: {analysis_summary}"
            )
            logger.info(log_entry)
        else:
            # On failure, log a simpler error message for clarity.
            logger.error(
                f"AGENT FAILED for PR #{pr_number} ({repo_name}). Reason: {result_message}"
            )

    except Exception as e:
        logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
        await broadcaster("log-error", f"Agent failed with error: {e}")
        return

---

await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
        
        # --- THIS IS THE CORE LOGIC CHANGE ---
        if not retrieved_docs:
            # --- CREATE MODE ---
            await broadcaster("log-step", "No relevant docs found. Switching to 'Create Mode'...")
            new_documentation = await creator_chain.ainvoke({
                "analysis_summary": analysis_summary,
                "git_diff": git_diff
            })
            # For creation, the source file is always the main knowledge base
            raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
        else:
            # --- UPDATE MODE ---
            # Make the confidence threshold configurable for easier testing
            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                return

---

def _append_to_file_sync(file_path: str, content: str):
    """Synchronous file append operation."""
    with open(file_path, "a", encoding="utf-8") as f:
        f.write(content)

# --- Updated Core Agent Logic ---

async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str, repo_name: str, pr_number: str, user_name: str):
    """This is the main 'brain' of the agent. It runs the full analysis-retrieval-rewrite pipeline."""
    
    if not retriever:
        print("Agent failed: AI components are not initialized.")
        await broadcaster("log-error", "Error: Agent AI components are not ready.")
        return

---

if "Error" in pr_url:
                result_message = f"Failed to create PR. Reason: {pr_url}"
                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
            else:
                result_message = f"Successfully created documentation PR: {pr_url}"
                await broadcaster("log-action", f"✅ Successfully created PR: {pr_url}")

        except Exception as e:
            result_message = f"Agent failed during PR creation with error: {e}"
            await broadcaster("log-error", f"Agent failed with error: {e}")
            # Log the exception traceback for debugging
            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
        
---
```


---

### AI-Generated Update (2025-11-16 11:51:33)

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.

## Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

## How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

## Agent Logic (`agent_logic.py`)

*   **`run_agent_analysis`:** This is the core of the agent.
    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### Relevant Code Changes
```diff
diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
index 36e9b76..2801bec 100644
--- a/backend/data/Knowledge_Base.md
+++ b/backend/data/Knowledge_Base.md
@@ -1,75 +1,3 @@
-# Doc-Ops Agent
-
-This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.
-
-## Purpose
-
-The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
-
-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
-
-## Core Technologies
-
-*   **Python:** The primary programming language for the backend application.
-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
-*   **python-dotenv:** Used to load environment variables from a `.env` file.
-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
-
-## How Components Work Together
-
-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
-
-1.  **GitHub Webhook (`/api/webhook/github`):**
-    *   This endpoint receives `POST` requests from GitHub.
-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
-
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
-
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
++*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
++*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
++*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
++*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
++
++## Core Technologies
++
++*   **Python:** The primary programming language for the backend application.
++*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
++*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
++*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
++*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
++*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
++*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
++*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
++*   **python-dotenv:** Used to load environment variables from a `.env` file.
++*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
++
++## How Components Work Together
++
++The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
++
++1.  **GitHub Webhook (`/api/webhook/github`):**
++    *   This endpoint receives `POST` requests from GitHub.
++    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
++    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
++    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
++    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
++    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
++
++ 2.  **Agent Logic (`agent_logic.py`):**
++     *   **`run_agent_analysis`:** This is the core of the agent.
++         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
++         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
++        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
++        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
++        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
++
++3.  **Vector Store (`vector_store.py`):**
++    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
++        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
++        *   It loads all `.md` files from the `data/` directory.
++        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
++        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
++        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
++    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
++    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
++
++4.  **Live Logging (`/api/stream/logs`):**
++    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
++    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
++
++5.  **Environment Variables:**
++    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
++    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
++
++In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
++
++---
++
++## Agent Logic (`agent_logic.py`)
++
++*   **`run_agent_analysis`:** This is the core of the agent.
++    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
++    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
++    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
++    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
++
++---
++
++# Step 9: Log the final result
++        if "Successfully" in result_message:
++            # On success, log the specific format you requested.
++            log_entry = (
++                f"This is an AI-generated documentation update for PR #{pr_number}, "
++                f"originally authored by @{user_name}.\n"
++                f"Original PR: '{pr_title}' AI Analysis: {analysis_summary}"
++            )
++            logger.info(log_entry)
++        else:
++            # On failure, log a simpler error message for clarity.
++            logger.error(
++                f"AGENT FAILED for PR #{pr_number} ({repo_name}). Reason: {result_message}"
++            )
++
++    except Exception as e:
++        logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
++        await broadcaster("log-error", f"Agent failed with error: {e}")
++        return
++
+```


---

### AI-Generated Update (2025-11-16 11:52:11)

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.

## Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

## How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

## Agent Logic (`agent_logic.py`)

*   **`run_agent_analysis`:** This is the core of the agent.
    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### Relevant Code Changes
```diff
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 6e327af..c6cbd2d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -1,299 +1,292 @@
-import os
-import asyncio
-import datetime
-import logging
-from github import Github
-from langchain_core.documents import Document
+# Doc-Ops Agent
 
-# --- Import our custom modules ---
-from llm_clients import (
-    get_analyzer_chain, 
-    get_rewriter_chain, 
-    format_docs_for_context,
-    get_creator_chain # <-- IMPORT THE NEW CHAIN
-)
-from vector_store import get_retriever, add_docs_to_store
+This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.
 
-# --- Load GitHub Token ---
-GITHUB_API_TOKEN = os.getenv("GITHUB_API_TOKEN")
+## Purpose
 
-# --- Initialize Global "AI" Components ---
-try:
-    print("Warming up AI components...")
-    retriever = get_retriever()
-    analyzer_chain = get_analyzer_chain()
-    rewriter_chain = get_rewriter_chain()
-    creator_chain = get_creator_chain() # <-- INITIALIZE THE NEW CHAIN
-    print("✅ AI components are ready.")
-except Exception as e:
-    print(f"🔥 FATAL ERROR: Failed to initialize AI components: {e}")
-    retriever, analyzer_chain, rewriter_chain, creator_chain = None, None, None, None
+The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
 
-# --- GitHub PR Creation Logic (Synchronous) ---
-def _create_github_pr_sync(logger, repo_name, pr_number, pr_title, pr_body, source_files, new_content):
-    """Creates a new branch, updates files, and opens a pull request. (BLOCKING)"""
-    # Get a logger instance within the thread to ensure it's configured
-    logger = logging.getLogger(__name__)
+*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
 
-    if not GITHUB_API_TOKEN:
-        return "Error: GITHUB_API_TOKEN not set."
+## Core Technologies
 
-    try:
-        # 1. Authenticate and get repo
-        g = Github(GITHUB_API_TOKEN)
-        repo = g.get_repo(repo_name)
-        
-        # 2. Get the default branch (e.g., 'main')
-        default_branch = repo.get_branch(repo.default_branch)
-        
-        # 3. Create a new branch name
-        new_branch_name = f"ai-docs-fix-pr-{pr_number}"
-        
-        # 4. Create the new branch from the default branch
-        try:
-            repo.create_git_ref(
-                ref=f"refs/heads/{new_branch_name}",
-                sha=default_branch.commit.sha
-            )
-        except Exception as e:
-            if "Reference already exists" in str(e):
-                logger.info(f"Branch '{new_branch_name}' already exists. Proceeding...")
-            else:
-                raise e
+*   **Python:** The primary programming language for the backend application.
+*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+*   **python-dotenv:** Used to load environment variables from a `.env` file.
+*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
 
-        # 5. Update the files
-        commit_message = f"docs: AI-generated updates for PR #{pr_number}"
-        
-        files_updated_count = 0
-        for file_path in source_files:
-            try:
-                # Get the file to get its SHA (required for update)
-                contents = repo.get_contents(file_path, ref=default_branch.name)
-                
-                # Update the file on the *new branch*
-                repo.update_file(
-                    path=contents.path,
-                    message=commit_message,
-                    content=new_content, # Using the full AI rewrite
-                    sha=contents.sha,
-                    branch=new_branch_name
-                )
-                logger.info(f"Successfully updated file: {file_path}")
-                files_updated_count += 1
-            except Exception as e:
-                logger.warning(f"Failed to update file {file_path}: {e}. Skipping...")
+## How Components Work Together
 
-        # 6. Create the Pull Request
-        if files_updated_count == 0:
-            logger.warning("No files were successfully updated, skipping PR creation.")
-            return "Error: No files were updated, so no PR was created."
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
 
-        pr = repo.create_pull(
-            title=pr_title,
-            body=pr_body,
-            head=new_branch_name,
-            base=repo.default_branch  # The branch to merge into
-        )
-        
-        print(f"Successfully created PR: {pr.html_url}")
-        return pr.html_url
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
 
-    except Exception as e:
-        logger.error(f"Error creating GitHub PR: {e}", exc_info=True)
-        return f"Error: {e}"
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
 
-# --- Async Wrapper for GitHub PR Creation ---
-async def create_github_pr_async(*args, **kwargs):
-    """
-    Runs the synchronous GitHub PR creation function in a separate thread
-    to avoid blocking the asyncio event loop.
-    """
-    # Use asyncio.to_thread which correctly handles passing kwargs to the thread.
-    # This is the modern replacement for loop.run_in_executor for this use case.
-    pr_url = await asyncio.to_thread(_create_github_pr_sync, *args, **kwargs)
-    return pr_url
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
 
-# --- NEW: Knowledge Base Update Logic ---
-async def update_knowledge_base(logger, broadcaster, new_documentation: str):
-    """Appends the newly generated documentation to the central knowledge base."""
-    knowledge_base_path = os.path.join(os.path.dirname(__file__), 'data', 'Knowledge_Base.md')
-    
-    try:
-        await broadcaster("log-step", "Updating central knowledge base...")
-        
-        # Create a formatted entry with a timestamp
-        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
-        update_entry = (
-            f"\n\n---\n\n"
-            f"### AI-Generated Update ({timestamp})\n\n"
-            f"{new_documentation}\n"
-        )
-        
-        # Append to the file asynchronously
-        loop = asyncio.get_running_loop()
-        await loop.run_in_executor(None, lambda: 
-            _append_to_file_sync(knowledge_base_path, update_entry)
-        )
-        await broadcaster("log-step", "✅ Knowledge base updated.")
-    except Exception as e:
-        logger.error(f"Failed to update knowledge base: {e}", exc_info=True)
-        await broadcaster("log-error", f"Could not update knowledge base: {e}")
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
 
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-        f.write(content)
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
 
-# --- Updated Core Agent Logic ---
-
-async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str, repo_name: str, pr_number: str, user_name: str):
-    """This is the main 'brain' of the agent. It runs the full analysis-retrieval-rewrite pipeline."""
-    
-    if not retriever:
-        print("Agent failed: AI components are not initialized.")
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
-    try:
-        # --- Step 1: Analyze the code diff ---
-        await broadcaster("log-step", f"Analyzing diff for PR: '{pr_title}'...")
-        analysis = await analyzer_chain.ainvoke({"git_diff": git_diff})
-        analysis_summary = analysis.get('analysis_summary', 'No summary provided.')
-        await broadcaster("log-step", f"Analysis: {analysis_summary}")
-
-        # --- Step 2: Gatekeeping ---
-        if not analysis.get('is_functional_change', False):
-            await broadcaster("log-skip", "Trivial change detected. No doc update needed.")
-            return
-
-        # --- Step 3: Retrieve relevant old docs ---
-        await broadcaster("log-step", "Functional change. Searching for relevant docs...")
-        
-        # --- THIS IS THE FIX: Perform a direct similarity search to guarantee scores ---
-        # The 'mmr' retriever is good for diversity but hides scores.
-        # We use the vectorstore directly to get scores for confidence checking.
-        docs_with_scores = await retriever.vectorstore.asimilarity_search_with_relevance_scores(
-            analysis_summary, k=5
-        )
-        
-        retrieved_docs = [doc for doc, score in docs_with_scores]
-        scores = [score for doc, score in docs_with_scores]
-        
-        # Calculate confidence score (highest similarity)
-        confidence_score = max(scores) if scores else 0.0
-        confidence_percent = f"{confidence_score * 100:.1f}%"
-
-        await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-        
-        # --- THIS IS THE CORE LOGIC CHANGE ---
-        if not retrieved_docs:
-            # --- CREATE MODE ---
-            await broadcaster("log-step", "No relevant docs found. Switching to 'Create Mode'...")
-            new_documentation = await creator_chain.ainvoke({
-                "analysis_summary": analysis_summary,
-                "git_diff": git_diff
-            })
-            # For creation, the source file is always the main knowledge base
-            raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-        else:
-            # --- UPDATE MODE ---
-            # Make the confidence threshold configurable for easier testing
-            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
-            await broadcaster("log-step", "Relevant docs found. Generating updates with LLM...")
-            old_docs_context = format_docs_for_context(retrieved_docs)
-            new_documentation = await rewriter_chain.ainvoke({
-                "analysis_summary": analysis_summary,
-                "old_docs_context": old_docs_context,
-                "git_diff": git_diff
-            })
-            # Get source files from the retrieved docs
-            raw_paths = list(set([doc.metadata.get('source') for doc in retrieved_docs]))
-        
-        await broadcaster("log-step", "✅ New documentation generated.")
-        
-        # --- Step 4: Update the Knowledge Base File ---
-        # The agent now "remembers" what it wrote by adding it to the central guide.
-        await update_knowledge_base(logger, broadcaster, new_documentation)
-
-        # --- Step 5: Incrementally update the vector store (More Efficient) ---
-        # Instead of rebuilding, we add the new doc directly to the index.
-        await broadcaster("log-step", "Incrementally updating vector store with new knowledge...")
-        new_doc = Document(page_content=new_documentation, metadata={"source": os.path.join('data', 'Knowledge_Base.md')})
-        await asyncio.to_thread(add_docs_to_store, [new_doc])
-        await broadcaster("log-step", "✅ Knowledge base is now up-to-date.")
-
-        # --- Step 7: Package the results for the PR ---
-        
-        # --- THIS IS THE FIX: Standardize path formatting for both modes ---
-        # This ensures `source_files` is always a clean list of strings.
-        formatted_paths = [path.replace("\\", "/") for path in raw_paths]
-        
-        # --- THIS IS THE CRITICAL FIX: Ensure all paths are relative to the repo root ---
-        source_files = []
-        for path in formatted_paths:
-            if not path.startswith("backend/"):
-                path = f"backend/{path}"
-            source_files.append(path)
-
-        pr_data = {
-            "new_content": new_documentation,
-            "source_files": source_files,
-            "pr_title": f"docs: AI update for '{pr_title}' (PR #{pr_number})",
-            "pr_body": f"This is an AI-generated documentation update for PR #{pr_number}, originally authored by **@{user_name}**.\n\n**Confidence Score:** {confidence_percent}\n\n**Original PR:** '{pr_title}'\n**AI Analysis:** {analysis_summary}"
-        }
-
-        # --- Step 8: Create the GitHub PR ---
-        await broadcaster("log-step", "Attempting to create GitHub pull request...")
-        
-        try:
-            pr_url = await create_github_pr_async(
-                repo_name=repo_name,
-                logger=logger,
-                pr_number=pr_number,
-                pr_title=pr_data["pr_title"],
-                pr_body=pr_data["pr_body"],
-                source_files=pr_data["source_files"],
-                new_content=pr_data["new_content"]
-            )
-
-            if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-            else:
-                result_message = f"Successfully created documentation PR: {pr_url}"
-                await broadcaster("log-action", f"✅ Successfully created PR: {pr_url}")
-
-        except Exception as e:
-            result_message = f"Agent failed during PR creation with error: {e}"
-            await broadcaster("log-error", f"Agent failed with error: {e}")
-            # Log the exception traceback for debugging
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-
-        # --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-                f"This is an AI-generated documentation update for PR #{pr_number}, "
-                f"originally authored by @{user_name}.\n"
-                f"Original PR: '{pr_title}' AI Analysis: {analysis_summary}"
-            )
-            logger.info(log_entry)
-        else:
-            # On failure, log a simpler error message for clarity.
-            logger.error(
-                f"AGENT FAILED for PR #{pr_number} ({repo_name}). Reason: {result_message}"
-            )
-
-    except Exception as e:
-        logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        await broadcaster("log-error", f"Agent failed with error: {e}")
-        return
-
-# --- Self-Test ---
-if __name__ == "__main__":
-    print("This file is not meant to be run directly.")
-    print("Please run 'uvicorn main:app --reload' from the 'backend' directory.")
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+## Agent Logic (`agent_logic.py`)
+
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..2801bec 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -1,75 +1,3 @@
+-# Doc-Ops Agent
+-
+-This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.
+-
+-## Purpose
+-
+-The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+-
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+-## Core Technologies
+-
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
+-
+-3.  **Vector Store (`vector_store.py`):**
+-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+-        *   It loads all `.md` files from the `data/` directory.
+-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+-
+-4.  **Live Logging (`/api/stream/logs`):**
+-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+-
+-5.  **Environment Variables:**
+-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+-
+-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
++*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
++*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
++*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
++*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
++
++## Core Technologies
++
++*   **Python:** The primary programming language for the backend application.
++*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
++*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
++*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
++*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
++*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
++*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
++*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
++*   **python-dotenv:** Used to load environment variables from a `.env` file.
++*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
++
++## How Components Work Together
++
++The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
++
++1.  **GitHub Webhook (`/api/webhook/github`):**
++    *   This endpoint receives `POST` requests from GitHub.
++    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
++    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
++    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
++    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
++    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
++
++ 2.  **Agent Logic (`agent_logic.py`):**
++     *   **`run_agent_analysis`:** This is the core of the agent.
++         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
++         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
++        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
++        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
++        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
++
++3.  **Vector Store (`vector_store.py`):**
++    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
++        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
++        *   It loads all `.md` files from the `data/` directory.
++        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
++        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
++        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
++    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
++    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
++
++4.  **Live Logging (`/api/stream/logs`):**
++    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
++    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
++
++5.  **Environment Variables:**
++    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
++    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
++
++In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+## Agent Logic (`agent_logic.py`)
+
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..2801bec 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -1,75 +1,3 @@
+-# Doc-Ops Agent
+-
+-This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.
+-
+-## Purpose
+-
+-The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+-
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+-## Core Technologies
+-
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
+-
+-3.  **Vector Store (`vector_store.py`):**
+-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+-        *   It loads all `.md` files from the `data/` directory.
+-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+-
+-4.  **Live Logging (`/api/stream/logs`):**
+-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+-
+-5.  **Environment Variables:**
+-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+-
+-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
++*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
++*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
++*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
++*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
++
++## Core Technologies
++
++*   **Python:** The primary programming language for the backend application.
++*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
++*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
++*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
++*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
++*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
++*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
++*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
++*   **python-dotenv:** Used to load environment variables from a `.env` file.
++*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
++
++## How Components Work Together
++
++The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
++
++1.  **GitHub Webhook (`/api/webhook/github`):**
++    *   This endpoint receives `POST` requests from GitHub.
++    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
++    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
++    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
++    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
++    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
++
++ 2.  **Agent Logic (`agent_logic.py`):**
++     *   **`run_agent_analysis`:** This is the core of the agent.
++         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
++         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
++        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
++        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
++        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
++
++3.  **Vector Store (`vector_store.py`):**
++    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
++        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
++        *   It loads all `.md` files from the `data/` directory.
++        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
++        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
++        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
++    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
++    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
++
++4.  **Live Logging (`/api/stream/logs`):**
++    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
++    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
++
++5.  **Environment Variables:**
++    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
++    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
++
++In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+## Agent Logic (`agent_logic.py`)
+
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..2801bec 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -1,75 +1,3 @@
+-# Doc-Ops Agent
+-
+-This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.
+-
+-## Purpose
+-
+-The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+-
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+-## Core Technologies
+-
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
+-
+-3.  **Vector Store (`vector_store.py`):**
+-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+-        *   It loads all `.md` files from the `data/` directory.
+-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+-
+-4.  **Live Logging (`/api/stream/logs`):**
+-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+-
+-5.  **Environment Variables:**
+-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+-
+-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
++*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
++*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
++*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
++*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
++
++## Core Technologies
++
++*   **Python:** The primary programming language for the backend application.
++*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
++*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
++*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
++*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
++*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
++*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
++*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
++*   **python-dotenv:** Used to load environment variables from a `.env` file.
++*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
++
++## How Components Work Together
++
++The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
++
++1.  **GitHub Webhook (`/api/webhook/github`):**
++    *   This endpoint receives `POST` requests from GitHub.
++    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
++    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
++    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
++    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
++    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
++
++ 2.  **Agent Logic (`agent_logic.py`):**
++     *   **`run_agent_analysis`:** This is the core of the agent.
++         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
++         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
++        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
++        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
++        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
++
++3.  **Vector Store (`vector_store.py`):**
++    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
++        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
++        *   It loads all `.md` files from the `data/` directory.
++        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
++        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
++        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
++    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
++    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
++
++4.  **Live Logging (`/api/stream/logs`):**
++    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
++    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
++
++5.  **Environment Variables:**
++    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
++    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
++
++In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..2801bec 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -1,75 +1,3 @@
+-# Doc-Ops Agent
+-
+-This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.
+-
+-## Purpose
+-
+-The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+-
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+-## Core Technologies
+-
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
+-
+-3.  **Vector Store (`vector_store.py`):**
+-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+-        *   It loads all `.md` files from the `data/` directory.
+-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+-
+-4.  **Live Logging (`/api/stream/logs`):**
+-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+-
+-5.  **Environment Variables:**
+-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+-
+-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
++*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
++*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
++*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
++*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
++
++## Core Technologies
++
++*   **Python:** The primary programming language for the backend application.
++*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
++*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
++*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
++*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
++*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
++*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
++*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
++*   **python-dotenv:** Used to load environment variables from a `.env` file.
++*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
++
++## How Components Work Together
++
++The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
++
++1.  **GitHub Webhook (`/api/webhook/github`):**
++    *   This endpoint receives `POST` requests from GitHub.
++    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
++    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
++    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
++    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
++    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
++
++ 2.  **Agent Logic (`agent_logic.py`):**
++     *   **`run_agent_analysis`:** This is the core of the agent.
++         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
++         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
++        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
++        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
++        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
++
++3.  **Vector Store (`vector_store.py`):**
++    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
++        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
++        *   It loads all `.md` files from the `data/` directory.
++        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
++        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
++        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
++    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
++    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
++
++4.  **Live Logging (`/api/stream/logs`):**
++    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
++    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
++
++5.  **Environment Variables:**
++    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
++    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
++
++In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..2801bec 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -1,75 +1,3 @@
+-# Doc-Ops Agent
+-
+-This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.
+-
+-## Purpose
+-
+-The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+-
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+-## Core Technologies
+-
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
+-
+-3.  **Vector Store (`vector_store.py`):**
+-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+-        *   It loads all `.md` files from the `data/` directory.
+-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+-
+-4.  **Live Logging (`/api/stream/logs`):**
+-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+-
+-5.  **Environment Variables:**
+-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+-
+-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### Relevant Code Changes
+```diff
+diff --git a/backend/agent_logic.py b/backend/agent_logic.py
+index 6e327af..c6cbd2d 100644
+--- a/backend/agent_logic.py
++++ b/backend/agent_logic.py
+@@ -1,299 +1,292 @@
+-import os
+-import asyncio
+-import datetime
+-import logging
+-from github import Github
+-from langchain_core.documents import Document
+# Doc-Ops Agent
 
-# --- Import our custom modules ---
-from llm_clients import (
-    get_analyzer_chain, 
-    get_rewriter_chain, 
-    format_docs_for_context,
-    get_creator_chain # <-- IMPORT THE NEW CHAIN
-)
-from vector_store import get_retriever, add_docs_to_store
+This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.
 
-# --- Load GitHub Token ---
-GITHUB_API_TOKEN = os.getenv("GITHUB_API_TOKEN")
+## Purpose
 
-# --- Initialize Global "AI" Components ---
-try:
-    print("Warming up AI components...")
-    retriever = get_retriever()
-    analyzer_chain = get_analyzer_chain()
-    rewriter_chain = get_rewriter_chain()
-    creator_chain = get_creator_chain() # <-- INITIALIZE THE NEW CHAIN
-    print("✅ AI components are ready.")
-except Exception as e:
-    print(f"🔥 FATAL ERROR: Failed to initialize AI components: {e}")
-    retriever, analyzer_chain, rewriter_chain, creator_chain = None, None, None, None
+The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
 
-# --- GitHub PR Creation Logic (Synchronous) ---
-def _create_github_pr_sync(logger, repo_name, pr_number, pr_title, pr_body, source_files, new_content):
-    """Creates a new branch, updates files, and opens a pull request. (BLOCKING)"""
-    # Get a logger instance within the thread to ensure it's configured
-    logger = logging.getLogger(__name__)
+*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
 
-    if not GITHUB_API_TOKEN:
-        return "Error: GITHUB_API_TOKEN not set."
+## Core Technologies
 
-    try:
-        # 1. Authenticate and get repo
-        g = Github(GITHUB_API_TOKEN)
-        repo = g.get_repo(repo_name)
-        
-        # 2. Get the default branch (e.g., 'main')
-        default_branch = repo.get_branch(repo.default_branch)
-        
-        # 3. Create a new branch name
-        new_branch_name = f"ai-docs-fix-pr-{pr_number}"
-        
-        # 4. Create the new branch from the default branch
-        try:
-            repo.create_git_ref(
-                ref=f"refs/heads/{new_branch_name}",
-                sha=default_branch.commit.sha
-            )
-        except Exception as e:
-            if "Reference already exists" in str(e):
-                logger.info(f"Branch '{new_branch_name}' already exists. Proceeding...")
-            else:
-                raise e
+*   **Python:** The primary programming language for the backend application.
+*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+*   **python-dotenv:** Used to load environment variables from a `.env` file.
+*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
 
-        # 5. Update the files
-        commit_message = f"docs: AI-generated updates for PR #{pr_number}"
-        
-        files_updated_count = 0
-        for file_path in source_files:
-            try:
-                # Get the file to get its SHA (required for update)
-                contents = repo.get_contents(file_path, ref=default_branch.name)
-                
-                # Update the file on the *new branch*
-                repo.update_file(
-                    path=contents.path,
-                    message=commit_message,
-                    content=new_content, # Using the full AI rewrite
-                    sha=contents.sha,
-                    branch=new_branch_name
-                )
-                logger.info(f"Successfully updated file: {file_path}")
-                files_updated_count += 1
-            except Exception as e:
-                logger.warning(f"Failed to update file {file_path}: {e}. Skipping...")
+## How Components Work Together
 
-        # 6. Create the Pull Request
-        if files_updated_count == 0:
-            logger.warning("No files were successfully updated, skipping PR creation.")
-            return "Error: No files were updated, so no PR was created."
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
 
-        pr = repo.create_pull(
-            title=pr_title,
-            body=pr_body,
-            head=new_branch_name,
-            base=repo.default_branch  # The branch to merge into
-        )
-        
-        print(f"Successfully created PR: {pr.html_url}")
-        return pr.html_url
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
 
-    except Exception as e:
-        logger.error(f"Error creating GitHub PR: {e}", exc_info=True)
-        return f"Error: {e}"
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
 
-# --- Async Wrapper for GitHub PR Creation ---
-async def create_github_pr_async(*args, **kwargs):
-    """
-    Runs the synchronous GitHub PR creation function in a separate thread
-    to avoid blocking the asyncio event loop.
-    """
-    # Use asyncio.to_thread which correctly handles passing kwargs to the thread.
-    # This is the modern replacement for loop.run_in_executor for this use case.
-    pr_url = await asyncio.to_thread(_create_github_pr_sync, *args, **kwargs)
-    return pr_url
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
 
-# --- NEW: Knowledge Base Update Logic ---
-async def update_knowledge_base(logger, broadcaster, new_documentation: str):
-    """Appends the newly generated documentation to the central knowledge base."""
-    knowledge_base_path = os.path.join(os.path.dirname(__file__), 'data', 'Knowledge_Base.md')
-    
-    try:
-        await broadcaster("log-step", "Updating central knowledge base...")
-        
-        # Create a formatted entry with a timestamp
-        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
-        update_entry = (
-            f"\n\n---\n\n"
-            f"### AI-Generated Update ({timestamp})\n\n"
-            f"{new_documentation}\n"
-        )
-        
-        # Append to the file asynchronously
-        loop = asyncio.get_running_loop()
-        await loop.run_in_executor(None, lambda: 
-            _append_to_file_sync(knowledge_base_path, update_entry)
-        )
-        await broadcaster("log-step", "✅ Knowledge base updated.")
-    except Exception as e:
-        logger.error(f"Failed to update knowledge base: {e}", exc_info=True)
-        await broadcaster("log-error", f"Could not update knowledge base: {e}")
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
 
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-        f.write(content)
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
 
-# --- Updated Core Agent Logic ---
-
-async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str, repo_name: str, pr_number: str, user_name: str):
-    """This is the main 'brain' of the agent. It runs the full analysis-retrieval-rewrite pipeline."""
-    
-    if not retriever:
-        print("Agent failed: AI components are not initialized.")
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
-    try:
-        # --- Step 1: Analyze the code diff ---
-        await broadcaster("log-step", f"Analyzing diff for PR: '{pr_title}'...")
-        analysis = await analyzer_chain.ainvoke({"git_diff": git_diff})
-        analysis_summary = analysis.get('analysis_summary', 'No summary provided.')
-        await broadcaster("log-step", f"Analysis: {analysis_summary}")
-
-        # --- Step 2: Gatekeeping ---
-        if not analysis.get('is_functional_change', False):
-            await broadcaster("log-skip", "Trivial change detected. No doc update needed.")
-            return
-
-        # --- Step 3: Retrieve relevant old docs ---
-        await broadcaster("log-step", "Functional change. Searching for relevant docs...")
-        
-        # --- THIS IS THE FIX: Perform a direct similarity search to guarantee scores ---
-        # The 'mmr' retriever is good for diversity but hides scores.
-        # We use the vectorstore directly to get scores for confidence checking.
-        docs_with_scores = await retriever.vectorstore.asimilarity_search_with_relevance_scores(
-            analysis_summary, k=5
-        )
-        
-        retrieved_docs = [doc for doc, score in docs_with_scores]
-        scores = [score for doc, score in docs_with_scores]
-        
-        # Calculate confidence score (highest similarity)
-        confidence_score = max(scores) if scores else 0.0
-        confidence_percent = f"{confidence_score * 100:.1f}%"
-
-        await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-        
-        # --- THIS IS THE CORE LOGIC CHANGE ---
-        if not retrieved_docs:
-            # --- CREATE MODE ---
-            await broadcaster("log-step", "No relevant docs found. Switching to 'Create Mode'...")
-            new_documentation = await creator_chain.ainvoke({
-                "analysis_summary": analysis_summary,
-                "git_diff": git_diff
-            })
-            # For creation, the source file is always the main knowledge base
-            raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-        else:
-            # --- UPDATE MODE ---
-            # Make the confidence threshold configurable for easier testing
-            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
-            await broadcaster("log-step", "Relevant docs found. Generating updates with LLM...")
-            old_docs_context = format_docs_for_context(retrieved_docs)
-            new_documentation = await rewriter_chain.ainvoke({
-                "analysis_summary": analysis_summary,
-                "old_docs_context": old_docs_context,
-                "git_diff": git_diff
-            })
-            # Get source files from the retrieved docs
-            raw_paths = list(set([doc.metadata.get('source') for doc in retrieved_docs]))
-        
-        await broadcaster("log-step", "✅ New documentation generated.")
-        
-        # --- Step 4: Update the Knowledge Base File ---
-        # The agent now "remembers" what it wrote by adding it to the central guide.
-        await update_knowledge_base(logger, broadcaster, new_documentation)
-
-        # --- Step 5: Incrementally update the vector store (More Efficient) ---
-        # Instead of rebuilding, we add the new doc directly to the index.
-        await broadcaster("log-step", "Incrementally updating vector store with new knowledge...")
-        new_doc = Document(page_content=new_documentation, metadata={"source": os.path.join('data', 'Knowledge_Base.md')})
-        await asyncio.to_thread(add_docs_to_store, [new_doc])
-        await broadcaster("log-step", "✅ Knowledge base is now up-to-date.")
-
-        # --- Step 7: Package the results for the PR ---
-        
-        # --- THIS IS THE FIX: Standardize path formatting for both modes ---
-        # This ensures `source_files` is always a clean list of strings.
-        formatted_paths = [path.replace("\\", "/") for path in raw_paths]
-        
-        # --- THIS IS THE CRITICAL FIX: Ensure all paths are relative to the repo root ---
-        source_files = []
-        for path in formatted_paths:
-            if not path.startswith("backend/"):
-                path = f"backend/{path}"
-            source_files.append(path)
-
-        pr_data = {
-            "new_content": new_documentation,
-            "source_files": source_files,
-            "pr_title": f"docs: AI update for '{pr_title}' (PR #{pr_number})",
-            "pr_body": f"This is an AI-generated documentation update for PR #{pr_number}, originally authored by **@{user_name}**.\n\n**Confidence Score:** {confidence_percent}\n\n**Original PR:** '{pr_title}'\n**AI Analysis:** {analysis_summary}"
-        }
-
-        # --- Step 8: Create the GitHub PR ---
-        await broadcaster("log-step", "Attempting to create GitHub pull request...")
-        
-        try:
-            pr_url = await create_github_pr_async(
-                repo_name=repo_name,
-                logger=logger,
-                pr_number=pr_number,
-                pr_title=pr_data["pr_title"],
-                pr_body=pr_data["pr_body"],
-                source_files=pr_data["source_files"],
-                new_content=pr_data["new_content"]
-            )
-
-            if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-            else:
-                result_message = f"Successfully created documentation PR: {pr_url}"
-                await broadcaster("log-action", f"✅ Successfully created PR: {pr_url}")
-
-        except Exception as e:
-            result_message = f"Agent failed during PR creation with error: {e}"
-            await broadcaster("log-error", f"Agent failed with error: {e}")
-            # Log the exception traceback for debugging
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-
-        # --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-                f"This is an AI-generated documentation update for PR #{pr_number}, "
-                f"originally authored by @{user_name}.\n"
-                f"Original PR: '{pr_title}' AI Analysis: {analysis_summary}"
-            )
-            logger.info(log_entry)
-        else:
-            # On failure, log a simpler error message for clarity.
-            logger.error(
-                f"AGENT FAILED for PR #{pr_number} ({repo_name}). Reason: {result_message}"
-            )
-
-    except Exception as e:
-        logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        await broadcaster("log-error", f"Agent failed with error: {e}")
-        return
-
-# --- Self-Test ---
-if __name__ == "__main__":
-    print("This file is not meant to be run directly.")
-    print("Please run 'uvicorn main:app --reload' from the 'backend' directory.")
+```


---

### AI-Generated Update (2025-11-16 11:52:22)

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

### AI-Generated Update (2025-11-16 11:47:09)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

---

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the


---

### AI-Generated Update (2025-11-16 11:53:16)

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

## Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.

## Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

## How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes
```diff
diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
index 36e9b76..a073f58 100644
--- a/backend/data/Knowledge_Base.md
+++ b/backend/data/Knowledge_Base.md
@@ -6,65 +6,53 @@
 
 The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
 
--*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
--*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
--*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
--*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
 ## Core Technologies
 
-*   **Python:** The primary programming language for the backend application.
-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
-*   **python-dotenv:** Used to load environment variables from a `.env` file.
-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
-
-## How Components Work Together
-
-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
-
-1.  **GitHub Webhook (`/api/webhook/github`):**
-    *   This endpoint receives `POST` requests from GitHub.
-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
-
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
---
-
-### AI-Generated Update (2025-11-16 11:47:09)
-
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-                f"This is an AI-generated documentation update for PR #{pr_number}, "
-                f"originally authored by @{user_name}.\n"
-                f"Original PR: '{pr_title}' AI Analysis: {analysis_summary}"
-            )
-            logger.info(log_entry)
-        else:
-            # On failure, log a simpler error message for clarity.
-            logger.error(
-                f"AGENT FAILED for PR #{pr_number} ({repo_name}). Reason: {result_message}"
-            )
-
-    except Exception as e:
-        logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        await broadcaster("log-error", f"Agent failed with error: {e}")
-        return
-
---- Snippet 3 (Source: agent_logic.py) ---
-await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-        
-        # --- THIS IS THE CORE LOGIC CHANGE ---
-        if not retrieved_docs:
-            # --- CREATE MODE ---
-            await broadcaster("log-step", "No relevant docs found. Switching to 'Create Mode'...")
-            new_documentation = await creator_chain.ainvoke({
-                "analysis_summary": analysis_summary,
-                "git_diff": git_diff
-            })
-            # For creation, the source file is always the main knowledge base
-            raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-        else:
-            # --- UPDATE MODE ---
-            # Make the confidence threshold configurable for easier testing
-            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
---- Snippet 4 (Source: agent_logic.py) ---
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-        f.write(content)
-
-# --- Updated Core Agent Logic ---
-
-async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str, repo_name: str, pr_number: str, user_name: str):
-    """This is the main 'brain' of the agent. It runs the full analysis-retrieval-rewrite pipeline."""
-    
-    if not retriever:
-        print("Agent failed: AI components are not initialized.")
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
---- Snippet 5 (Source: agent_logic.py) ---
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-            else:
-                result_message = f"Successfully created documentation PR: {pr_url}"
-                await broadcaster("log-action", f"✅ Successfully created PR: {pr_url}")
-
-        except Exception as e:
-            result_message = f"Agent failed during PR creation with error: {e}"
-            await broadcaster("log-error", f"Agent failed with error: {e}")
-            # Log the exception traceback for debugging
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
- ### Relevant Code Changes
- ```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
- ```
-
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
-
-```
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+ 3.  **Vector Store (`vector_store.py`):**
+     *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+         *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+         *   It loads all `.md` files from the `data/` directory.
+         *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+         *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+         *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+     *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+     *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+ 
+4.  **Live Logging (`/api/stream/logs`):**
+     *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+     *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+ 
+5.  **Environment Variables:**
+     *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+     *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+     *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+ 
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+-## How Components Work Together
+-
+-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+-
+-1.  **GitHub Webhook (`/api/webhook/github`):**
+-    *   This endpoint receives `POST` requests from GitHub.
+-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----
+-
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
  
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector


---

### AI-Generated Update (2025-11-16 11:53:49)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.

---
### Relevant Code Changes
```diff
diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
index 36e9b76..4ec729c 100644
--- a/backend/data/Knowledge_Base.md
+++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+ 
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+ 
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -140,44 +133,13 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_


---

### AI-Generated Update (2025-11-16 11:53:56)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.

---
### Relevant Code Changes
```diff
diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
index 36e9b76..4ec729c 100644
--- a/backend/data/Knowledge_Base.md
+++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@ The system is designed around a central FastAPI application that listens for Git
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                 return
- 
-```
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
++++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
+## Agent Logic (`agent_logic.py`)
 
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
 
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+---
 
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
+# Step 9: Log the final result
         if "Successfully" in result_message:
             # On success, log the specific format you requested.
             log_entry = (
@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
---- Snippet 3 (Source: agent_logic.py) ---
+---
+
 def _append_to_file_sync(file_path: str, content: str):
     """Synchronous file append operation."""
     with open(file_path, "a", encoding="utf-8") as f:
@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
         await broadcaster("log-error", "Error: Agent AI components are not ready.")
         return
 
---- Snippet 4 (Source: agent_logic.py) ---
+---
+
 if "Error" in pr_url:
                 result_message = f"Failed to create PR. Reason: {pr_url}"
                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
@@ -139,44 +132,10 @@ if "Error" in pr_url:
             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
         
 ---
-### Relevant Code Changes
-```diff
-diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
-index 31b5e7c..629049c 100644
---- a/backend/USER_GUIDE.md
-+++ b/backend/USER_GUIDE.md
-@@ -86,6 +86,9 @@ The backend is a Python FastAPI application.
- 
-     # Your OpenAI API key
-     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-+
-+    # (Optional) The minimum confidence score required to update a document
-+    CONFIDENCE_THRESHOLD=0.2
-     ```
- 
- ### Step 3: Frontend Setup
-@@ -203,7 +206,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
-     *   **Build Command**: `pip install -r requirements.txt`
-         *   This is usually the default and is correct.
-     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
--        *   Use `10000` for the port as recommended by Render.
-+        *   Use the port recommended by Render (e.g., `10000`).
- 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
- 4.  **Deploy**: Trigger a manual deploy.
- 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
-diff --git a/backend/agent_logic.py b/backend/agent_logic.py
-index 194eafb..3b3f53d 100644
---- a/backend/agent_logic.py
-+++ b/backend/agent_logic.py
-@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
-         else:
-             # --- UPDATE MODE ---
--            if confidence_score < 0.2: # Gatekeeping based on confidence
-+            # Make the confidence threshold configurable for easier testing
-+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
-+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
-


---

### AI-Generated Update (2025-11-16 11:55:34)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

## How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes
```diff
diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
index 36e9b76..a073f58 100644
--- a/backend/data/Knowledge_Base.md
+++ b/backend/data/Knowledge_Base.md
@@ -6,65 +6,53 @@
 
 The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
 
-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
-
 ## Core Technologies
 
-*   **Python:** The primary programming language for the backend application.
-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
-*   **python-dotenv:** Used to load environment variables from a `.env` file.
-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
-
-## How Components Work Together
-
-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
-
-1.  **GitHub Webhook (`/api/webhook/github`):**
-    *   This endpoint receives `POST` requests from GitHub.
-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
-
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+    *   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+    *   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+    *   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+    *   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+
+## Core Technologies
+
+    *   **Python:** The primary programming language for the backend application.
+    *   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+    *   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+    *   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+    *   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+    *   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+    *   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+    *   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+    *   **python-dotenv:** Used to load environment variables from a `.env` file.
+    *   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+
+## How Components Work Together
+
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---
+
+
+---
+ ### Relevant Code Changes
+ ```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..a073f58 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -6,65 +6,53 @@
+ 
+ The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+ 
+-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+-
+ ## Core Technologies
+ 
+-*   **Python:** The primary programming language for the backend application.
+-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+-*   **python-dotenv:** Used to load environment variables from a `.env` file.
+-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+-
+ ## How Components Work Together
+ 
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_


---

### AI-Generated Update (2025-11-16 11:56:16)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

## How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes
```diff
diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
index 36e9b76..a073f58 100644
--- a/backend/data/Knowledge_Base.md
+++ b/backend/data/Knowledge_Base.md
@@ -6,65 +6,53 @@
 
 The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
 
-*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
-*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
-*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
-*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
-
- ## Core Technologies
- 
-*   **Python:** The primary programming language for the backend application.
-*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
-*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
-*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
-*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
-*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
-*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
-*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
-*   **python-dotenv:** Used to load environment variables from a `.env` file.
-*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
-
- ## How Components Work Together
- 
-The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
-
-1.  **GitHub Webhook (`/api/webhook/github`):**
-    *   This endpoint receives `POST` requests from GitHub.
-    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
-    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
-    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
-    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
-    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
-
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
-3.  **Vector Store (`vector_store.py`):**
-    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
-        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
-        *   It loads all `.md` files from the `data/` directory.
-        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
-        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
-        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
-    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
-    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.
-
-4.  **Live Logging (`/api/stream/logs`):**
-    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
-    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
-
-5.  **Environment Variables:**
-    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
-    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
-    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.
-
-In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
+index 31b5e7c..629049c 100644
+--- a/backend/USER_GUIDE.md
++++/backend/USER_GUIDE.md
+@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
+     *   **Build Command**: `pip install -r requirements.txt`
+         *   This is usually the default and is correct.
+     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
+-        *   Use `10000` for the port as recommended by Render.
++        *   Use the port recommended by Render (e.g., `10000`).
+ 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
+ 4.  **Deploy**: Trigger a manual deploy.
+ 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
+diff --git a/backend/agent_logic.py b/backend/agent_logic.py
+index 194eafb..3b3f53d 100644
+--- a/backend/agent_logic.py
++++/backend/agent_logic.py
+@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+         else:
+             # --- UPDATE MODE ---
+-            if confidence_score < 0.2: # Gatekeeping based on confidence
++            # Make the confidence threshold configurable for easier testing
++            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
++            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
```


---

### AI-Generated Update (2025-11-16 11:56:28)

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.

---
### Relevant Code Changes
```diff
diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
index 36e9b76..4ec729c 100644
--- a/backend/data/Knowledge_Base.md
+++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
-
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
- 2.  **Agent Logic (`agent_logic.py`):**
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+## Agent Logic (`agent_logic.py`)
+
+*   **`run_agent_analysis`:** This is the core of the agent.
+    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+         if "Successfully" in result_message:
+             # On success, log the specific format you requested.
+             log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                 return
+ 
+---- Snippet 3 (Source: agent_logic.py) ---
+---
+
+ def _append_to_file_sync(file_path: str, content: str):
+     """Synchronous file append operation."""
+     with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+         await broadcaster("log-error", "Error: Agent AI components are not ready.")
+         return
+ 
+---- Snippet 4 (Source: agent_logic.py) ---
+---
+
+ if "Error" in pr_url:
+                 result_message = f"Failed to create PR. Reason: {pr_url}"
+                 await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+             logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+         
+ ---
+### Relevant Code Changes
+```diff
+diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+index 36e9b76..4ec729c 100644
+--- a/backend/data/Knowledge_Base.md
+++++ b/backend/data/Knowledge_Base.md
+@@ -40,7 +40,7 @@
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+ 2.  **Agent Logic (`agent_logic.py`):**
+     *   **`run_agent_analysis`:** This is the core of the agent.
+         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+         *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.


---

### AI-Generated Update (2025-11-16 11:56:33)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

## How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes

```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
     *   **Build Command**: `pip install -r requirements.txt`
         *   This is usually the default and is correct.
     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-        *   Use `10000` for the port as recommended by Render.
+        *   Use the port recommended by Render (e.g., `10000`).
 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
 4.  **Deploy**: Trigger a manual deploy.
 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
         else:
             # --- UPDATE MODE ---
-            if confidence_score < 0.2: # Gatekeeping based on confidence
+            # Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
```


---

### AI-Generated Update (2025-11-16 11:56:34)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

## How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search with a score threshold.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes

```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
     *   **Build Command**: `pip install -r requirements.txt`
         *   This is usually the default and is correct.
     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-        *   Use `10000` for the port as recommended by Render.
+        *   Use the port recommended by Render (e.g., `10000`).
 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
 4.  **Deploy**: Trigger a manual deploy.
 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
         else:
             # --- UPDATE MODE ---
-            if confidence_score < 0.2: # Gatekeeping based on confidence
+            # Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
```


---

### AI-Generated Update (2025-11-16 11:56:50)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Agent Logic (`agent_logic.py`)

*   **`run_agent_analysis`:** This is the core of the agent.
    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.

---
### Relevant Code Changes
```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
     *   **Build Command**: `pip install -r requirements.txt`
         *   This is usually the default and is correct.
     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-
+    # (Optional) The minimum confidence score required to update a document
     # Your OpenAI API key
     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-
+    CONFIDENCE_THRESHOLD=0.2
     ```
 
 ### Step 3: Frontend Setup
@@ -199,5 +199,4 @@ index 194eafb..3b3f53d 100644
 +            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
- 
-```
+
```


---

### AI-Generated Update (2025-11-16 11:56:53)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Core Technologies

The system is designed around a central FastAPI application that listens for Git events. Key components include:

1.  **FastAPI Backend (`main.py`):**
    *   Handles incoming GitHub webhooks (e.g., push events).
    *   Orchestrates the documentation generation and update process.
    *   Exposes API endpoints for potential future integrations.

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index. It takes a list of file paths, reads their content, and generates embeddings using a specified model. These embeddings are then stored in a FAISS index file (`docs.index`) and a mapping of document content to their embeddings (`doc_map.json`).
    *   **`load_vector_store`:** This function loads the existing FAISS index and the document map from disk, allowing the agent to perform similarity searches.

4.  **Broadcaster (`broadcaster.py`):**
    *   A utility for sending real-time status updates (logs, errors, actions) from the backend to connected clients (e.g., a frontend dashboard) via WebSockets.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes

```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
     *   **Build Command**: `pip install -r requirements.txt`
         *   This is usually the default and is correct.
     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-        ```
+```
 
     # Your OpenAI API key
     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
@@ -199,5 +199,4 @@ index 194eafb..3b3f53d 100644
             if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
-
```
```


---

### AI-Generated Update (2025-11-16 11:57:11)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Agent Logic (`agent_logic.py`)

*   **`run_agent_analysis`:** This is the core of the agent.
    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.

---
### Relevant Code Changes
```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
     *   **Build Command**: `pip install -r requirements.txt`
         *   This is usually the default and is correct.
     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-
+    # (Optional) The minimum confidence score required to update a document
     # Your OpenAI API key
     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
-
+    CONFIDENCE_THRESHOLD=0.2
     ```
 
 ### Step 3: Frontend Setup
@@ -199,5 +199,4 @@ index 194eafb..3b3f53d 100644
 +            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
- 
-```
+
```


---

### AI-Generated Update (2025-11-16 11:57:13)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **FastAPI Backend (`main.py`):**
    *   Handles incoming GitHub webhooks (e.g., push events).
    *   Orchestrates the documentation generation and update process.
    *   Exposes API endpoints for potential future integrations.

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index. It takes a list of file paths, reads their content, and generates embeddings using a specified model. These embeddings are then stored in a FAISS index file (`docs.index`) and a mapping of document content to their embeddings (`doc_map.json`).
    *   **`load_vector_store`:** This function loads the existing FAISS index and the document map from disk, allowing the agent to perform similarity searches.

4.  **Broadcaster (`broadcaster.py`):**
    *   A utility for sending real-time status updates (logs, errors, actions) from the backend to connected clients (e.g., a frontend dashboard) via WebSockets.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes
```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
     # Your OpenAI API key
     OPENAI_API_KEY="sk-YourOpenAIKeyHere"
 
-    # (Optional) The minimum confidence score required to update a document
+    # (Optional) The minimum confidence score required to update a document. Defaults to 0.2.
     CONFIDENCE_THRESHOLD=0.2
     ```
 
@@ -203,7 +203,7 @@ To deploy the backend to a persistent cloud service like Render, follow these st
     *   **Build Command**: `pip install -r requirements.txt`
         *   This is usually the default and is correct.
     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-        *   Use `10000` for the port as recommended by Render.
+        *   Use the port recommended by Render (e.g., `10000`).
 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
 4.  **Deploy**: Trigger a manual deploy.
 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
         else:
             # --- UPDATE MODE ---
-            if confidence_score < 0.2: # Gatekeeping based on confidence
+            # Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
```


---

### AI-Generated Update (2025-11-16 11:57:13)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

## How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index. It takes a list of file paths, reads their content, and generates embeddings using a specified model. These embeddings are then stored in a FAISS index file (`docs.index`) and a mapping of document content to their embeddings (`doc_map.json`).
    *   **`load_vector_store`:** This function loads the existing FAISS index and the document map from disk, allowing the agent to perform similarity searches.

4.  **Broadcaster (`broadcaster.py`):**
    *   A utility for sending real-time status updates (logs, errors, actions) from the backend to connected clients (e.g., a frontend dashboard) via WebSockets.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes
```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
      *   **Build Command**: `pip install -r requirements.txt`
          *   This is usually the default and is correct.
      *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-        *   Use `10000` for the port as recommended by Render.
+        *   Use the port recommended by Render (e.g., `10000`).
  3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
  4.  **Deploy**: Trigger a manual deploy.
  5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -199,5 +199,4 @@
             if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
-
 ```
```


---

### AI-Generated Update (2025-11-16 11:57:26)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Agent Logic (`agent_logic.py`)

*   **`run_agent_analysis`:** This is the core of the agent.
    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.

---

### Relevant Code Changes

```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
     *   **Build Command**: `pip install -r requirements.txt`
         *   This is usually the default and is correct.
     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-        *   Use `10000` for the port as recommended by Render.
+        *   Use the port recommended by Render (e.g., `10000`).
 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
 4.  **Deploy**: Trigger a manual deploy.
 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -199,5 +199,4 @@ index 194eafb..3b3f53d 100644
 +            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
- 
-```
+
```


---

### AI-Generated Update (2025-11-16 11:57:28)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Agent Logic (`agent_logic.py`)

*   **`run_agent_analysis`:** This is the core of the agent.
    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.

---

### Relevant Code Changes

```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
     *   **Build Command**: `pip install -r requirements.txt`
         *   This is usually the default and is correct.
     *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-        *   Use `10000` for the port as recommended by Render.
+        *   Use the port recommended by Render (e.g., `10000`).
 3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
 4.  **Deploy**: Trigger a manual deploy.
 5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -197,7 +197,9 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
         else:
             # --- UPDATE MODE ---
-            if confidence_score < 0.2: # Gatekeeping based on confidence
+            # Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return

```


---

### AI-Generated Update (2025-11-16 11:57:29)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Core Technologies

-   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints.
-   **LangChain:** A framework for developing applications powered by language models. It allows for the chaining of different components, such as LLMs, prompt templates, and retrievers.
-   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to create a searchable index of the documentation.
-   **HuggingFace Embeddings:** Utilized for generating vector embeddings of text data, enabling semantic understanding and retrieval.
-   **Docker:** Used for containerizing the application, ensuring consistent environments across development and deployment.
-   **GitHub Webhooks:** To trigger the agent when code changes occur in a repository.

## Agent Logic (`agent_logic.py`)

*   **`run_agent_analysis`:** This is the core of the agent.
    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
    *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
    *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

## Vector Store (`vector_store.py`)

*   **`create_vector_store`:** This function is responsible for building the FAISS index. It takes a list of file paths, reads their content, and generates embeddings using a specified model. These embeddings are then stored in a FAISS index file (`docs.index`) and a mapping of document content to their embeddings (`doc_map.json`).
*   **`load_vector_store`:** This function loads the existing FAISS index and the document map from disk, allowing the agent to perform similarity searches.

## Broadcaster (`broadcaster.py`)

*   A utility for sending real-time status updates (logs, errors, actions) from the backend to connected clients (e.g., a frontend dashboard) via WebSockets.

## Environment Variables

*   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
*   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
*   `OPENAI_API_KEY`: Your OpenAI API key.
*   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes

```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
      *   **Build Command**: `pip install -r requirements.txt`
          *   This is usually the default and is correct.
      *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-         *   Use `10000` for the port as recommended by Render.
+         *   Use the port recommended by Render (e.g., `10000`).
  3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
  4.  **Deploy**: Trigger a manual deploy.
  5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -199,5 +199,4 @@
             if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
-
 ```
```


---

### AI-Generated Update (2025-11-16 11:57:29)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Core Technologies

The system is designed around a central FastAPI application that listens for Git events. Key components include:

1.  **FastAPI Backend (`main.py`):**
    *   Handles incoming webhooks from GitHub.
    *   Orchestrates the documentation generation and update process.
    *   Provides API endpoints for status updates and logs.

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index. It takes a list of file paths, reads their content, and generates embeddings using a specified model. These embeddings are then stored in a FAISS index file (`docs.index`) and a mapping of document content to their embeddings (`doc_map.json`).
    *   **`load_vector_store`:** This function loads the existing FAISS index and the document map from disk, allowing the agent to perform similarity searches.

4.  **Broadcaster (`broadcaster.py`):**
    *   A utility for sending real-time status updates (logs, errors, actions) from the backend to connected clients (e.g., a frontend dashboard) via WebSockets.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
    *   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

## Deployment with Render

To deploy the backend to a persistent cloud service like Render, follow these steps:

1.  **Create a New Service:** In your Render dashboard, click "New +" and select "Web Service".
2.  **Connect Your Repository:** Connect your GitHub repository.
3.  **Configure Build and Start Commands:**
    *   **Build Command**: `pip install -r requirements.txt`
        *   This is usually the default and is correct.
    *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
        *   Use the port recommended by Render (e.g., `10000`).
4.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
5.  **Deploy**: Trigger a manual deploy.
6.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).

---

### Relevant Code Changes
```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
      *   **Build Command**: `pip install -r requirements.txt`
          *   This is usually the default and is correct.
      *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-         *   Use `10000` for the port as recommended by Render.
+         *   Use the port recommended by Render (e.g., `10000`).
  3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
  4.  **Deploy**: Trigger a manual deploy.
  5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -199,5 +199,4 @@
             if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
-
 ```
```


---

### AI-Generated Update (2025-11-16 11:58:56)

## Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

### Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.

### Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

### How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
    *   `OPENAI_API_KEY`: Your OpenAI API key.
    *   `CONFIDENCE_THRESHOLD`: (Optional) The minimum confidence score required to update a document. Defaults to 0.2.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes
```diff
diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
index 36e9b76..4ec729c 100644
--- a/backend/data/Knowledge_Base.md
+++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
 
 ---
 
-### AI-Generated Update (2025-11-16 11:47:09)
-
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
---- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
---- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
---- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
---- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 1 (Source: data\Knowledge_Base.md) ---
-2.  **Agent Logic (`agent_logic.py`):**
-    *   **`run_agent_analysis`:** This is the core of the agent.
-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-
----- Snippet 2 (Source: agent_logic.py) ---
-# --- Step 9: Log the final result ---
-        if "Successfully" in result_message:
-            # On success, log the specific format you requested.
-            log_entry = (
-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
-                return
-
----- Snippet 3 (Source: agent_logic.py) ---
---
-
-def _append_to_file_sync(file_path: str, content: str):
-    """Synchronous file append operation."""
-    with open(file_path, "a", encoding="utf-8") as f:
-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
-        return
-
----- Snippet 4 (Source: agent_logic.py) ---
---
-
-if "Error" in pr_url:
-                result_message = f"Failed to create PR. Reason: {pr_url}"
-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
-@@ -139,44 +132,10 @@ if "Error" in pr_url:
-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
-        
----
-### Relevant Code Changes
-```diff
-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
-index 36e9b76..4ec729c 100644
---- a/backend/data/Knowledge_Base.md
-+++ b/backend/data/Knowledge_Base.md
-@@ -40,7 +40,7 @@
-     *   **`run_agent_analysis`:** This is the core of the agent.
-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
- 
- ---
- 
--### AI-Generated Update (2025-11-16 11:47:09)
--
--2.  **Agent Logic (`agent_logic.py`):**
--    *   **`run_agent_analysis`:** This is the core of the agent.
--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
--        *


---

### AI-Generated Update (2025-11-16 12:00:13)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Core Technologies

*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints.
*   **LangChain:** A framework for developing applications powered by language models. It allows for the chaining of different components, such as LLMs, prompt templates, and retrievers.
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to create a searchable index of the documentation.
*   **HuggingFace Embeddings:** Utilized for generating vector embeddings of text data, enabling semantic understanding and retrieval.
*   **Docker:** Used for containerizing the application, ensuring consistent environments across development and deployment.
*   **GitHub Webhooks:** To trigger the agent when code changes occur in a repository.

## Agent Logic (`agent_logic.py`)

*   **`run_agent_analysis`:** This is the core of the agent.
    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
    *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
    *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

## Vector Store (`vector_store.py`)

*   **`create_vector_store`:** This function is responsible for building the FAISS index. It takes a list of file paths, reads their content, and generates embeddings using a specified model. These embeddings are then stored in a FAISS index file (`docs.index`) and a mapping of document content to their embeddings (`doc_map.json`).
*   **`load_vector_store`:** This function loads the existing FAISS index and the document map from disk, allowing the agent to perform similarity searches.

## Broadcaster (`broadcaster.py`)

*   A utility for sending real-time status updates (logs, errors, actions) from the backend to connected clients (e.g., a frontend dashboard) via WebSockets.

## Environment Variables

*   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
*   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
*   `OPENAI_API_KEY`: Your OpenAI API key.
*   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

---

### Relevant Code Changes

```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
      *   **Build Command**: `pip install -r requirements.txt`
          *   This is usually the default and is correct.
      *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-         *   Use `10000` for the port as recommended by Render.
+         *   Use the port recommended by Render (e.g., `10000`).
  3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
  4.  **Deploy**: Trigger a manual deploy.
  5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -199,5 +199,4 @@
             if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
-
 ```
```


---

### AI-Generated Update (2025-11-16 12:00:30)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Core Technologies

*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints.
*   **LangChain:** A framework for developing applications powered by language models. It allows for the chaining of different components, such as LLMs, prompt templates, and retrievers.
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to create a searchable index of the documentation.
*   **HuggingFace Embeddings:** Utilized for generating vector embeddings of text data, enabling semantic understanding and retrieval.
*   **Docker:** Used for containerizing the application, ensuring consistent environments across development and deployment.
*   **GitHub Webhooks:** To trigger the agent when code changes occur in a repository.

## Agent Logic (`agent_logic.py`)

*   **`run_agent_analysis`:** This is the core of the agent.
    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
    *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
    *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

## Vector Store (`vector_store.py`)

*   **`create_vector_store`:** This function is responsible for building the FAISS index. It takes a list of file paths, reads their content, and generates embeddings using a specified model. These embeddings are then stored in a FAISS index file (`docs.index`) and a mapping of document content to their embeddings (`doc_map.json`).
*   **`load_vector_store`:** This function loads the existing FAISS index and the document map from disk, allowing the agent to perform similarity searches.

## Broadcaster (`broadcaster.py`)

*   A utility for sending real-time status updates (logs, errors, actions) from the backend to connected clients (e.g., a frontend dashboard) via WebSockets.

## Environment Variables

*   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
*   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
*   `OPENAI_API_KEY`: Your OpenAI API key.
*   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

---

### Relevant Code Changes

```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
      *   **Build Command**: `pip install -r requirements.txt`
          *   This is usually the default and is correct.
      *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-         *   Use `10000` for the port as recommended by Render.
+         *   Use the port recommended by Render (e.g., `10000`).
  3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
  4.  **Deploy**: Trigger a manual deploy.
  5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -199,5 +199,4 @@
             if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
-
 ```
```


---

### AI-Generated Update (2025-11-16 12:00:30)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Core Technologies

*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints.
*   **LangChain:** A framework for developing applications powered by language models. It allows for the chaining of different components, such as LLMs, prompt templates, and retrievers.
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to create a searchable index of the documentation.
*   **HuggingFace Embeddings:** Utilized for generating vector embeddings of text data, enabling semantic understanding and retrieval.
*   **Docker:** Used for containerizing the application, ensuring consistent environments across development and deployment.
*   **GitHub Webhooks:** To trigger the agent when code changes occur in a repository.

## Agent Logic (`agent_logic.py`)

*   **`run_agent_analysis`:** This is the core of the agent.
    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
    *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
    *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

## Vector Store (`vector_store.py`)

*   **`create_vector_store`:** This function is responsible for building the FAISS index. It takes a list of file paths, reads their content, and generates embeddings using a specified model. These embeddings are then stored in a FAISS index file (`docs.index`) and a mapping of document content to their embeddings (`doc_map.json`).
*   **`load_vector_store`:** This function loads the existing FAISS index and the document map from disk, allowing the agent to perform similarity searches.

## Broadcaster (`broadcaster.py`)

*   A utility for sending real-time status updates (logs, errors, actions) from the backend to connected clients (e.g., a frontend dashboard) via WebSockets.

## Environment Variables

*   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
*   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
*   `OPENAI_API_KEY`: Your OpenAI API key.
*   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

---

### Relevant Code Changes

```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
      *   **Build Command**: `pip install -r requirements.txt`
          *   This is usually the default and is correct.
      *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-         *   Use `10000` for the port as recommended by Render.
+         *   Use the port recommended by Render (e.g., `10000`).
  3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
  4.  **Deploy**: Trigger a manual deploy.
  5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -199,5 +199,4 @@
             if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
-
 ```
```


---

### AI-Generated Update (2025-11-16 12:00:46)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Core Technologies

*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints.
*   **LangChain:** A framework for developing applications powered by language models. It allows for the chaining of different components, such as LLMs, prompt templates, and retrievers.
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to create a searchable index of the documentation.
*   **HuggingFace Embeddings:** Utilized for generating vector embeddings of text data, enabling semantic understanding and retrieval.
*   **Docker:** Used for containerizing the application, ensuring consistent environments across development and deployment.
*   **GitHub Webhooks:** To trigger the agent when code changes occur in a repository.

## Agent Logic (`agent_logic.py`)

*   **`run_agent_analysis`:** This is the core of the agent.
    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
    *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
    *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

## Vector Store (`vector_store.py`)

*   **`create_vector_store`:** This function is responsible for building the FAISS index. It takes a list of file paths, reads their content, and generates embeddings using a specified model. These embeddings are then stored in a FAISS index file (`docs.index`) and a mapping of document content to their embeddings (`doc_map.json`).
*   **`load_vector_store`:** This function loads the existing FAISS index and the document map from disk, allowing the agent to perform similarity searches.

## Broadcaster (`broadcaster.py`)

*   A utility for sending real-time status updates (logs, errors, actions) from the backend to connected clients (e.g., a frontend dashboard) via WebSockets.

## Environment Variables

*   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
*   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
*   `OPENAI_API_KEY`: Your OpenAI API key.
*   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

---

### Relevant Code Changes

```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
      *   **Build Command**: `pip install -r requirements.txt`
          *   This is usually the default and is correct.
      *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-         *   Use `10000` for the port as recommended by Render.
+         *   Use the port recommended by Render (e.g., `10000`).
  3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
  4.  **Deploy**: Trigger a manual deploy.
  5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -199,5 +199,4 @@
             if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
-
 ```
```


---

### AI-Generated Update (2025-11-16 12:00:47)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Core Technologies

*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints.
*   **LangChain:** A framework for developing applications powered by language models. It allows for the chaining of different components, such as LLMs, prompt templates, and retrievers.
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to create a searchable index of the documentation.
*   **HuggingFace Embeddings:** Utilized for generating vector embeddings of text data, enabling semantic understanding and retrieval.
*   **Docker:** Used for containerizing the application, ensuring consistent environments across development and deployment.
*   **GitHub Webhooks:** To trigger the agent when code changes occur in a repository.

## Agent Logic (`agent_logic.py`)

*   **`run_agent_analysis`:** This is the core of the agent.
    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
    *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
    *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

## Vector Store (`vector_store.py`)

*   **`create_vector_store`:** This function is responsible for building the FAISS index. It takes a list of file paths, reads their content, and generates embeddings using a specified model. These embeddings are then stored in a FAISS index file (`docs.index`) and a mapping of document content to their embeddings (`doc_map.json`).
*   **`load_vector_store`:** This function loads the existing FAISS index and the document map from disk, allowing the agent to perform similarity searches.

## Broadcaster (`broadcaster.py`)

*   A utility for sending real-time status updates (logs, errors, actions) from the backend to connected clients (e.g., a frontend dashboard) via WebSockets.

## Environment Variables

*   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
*   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
*   `OPENAI_API_KEY`: Your OpenAI API key.
*   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

---

### Relevant Code Changes

```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
      *   **Build Command**: `pip install -r requirements.txt`
          *   This is usually the default and is correct.
      *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-         *   Use `10000` for the port as recommended by Render.
+         *   Use the port recommended by Render (e.g., `10000`).
  3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
  4.  **Deploy**: Trigger a manual deploy.
  5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -199,5 +199,4 @@
             if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
-
 ```
```


---

### AI-Generated Update (2025-11-16 12:01:03)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Core Technologies

*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints.
*   **LangChain:** A framework for developing applications powered by language models. It allows for the chaining of different components, such as LLMs, prompt templates, and retrievers.
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to create a searchable index of the documentation.
*   **HuggingFace Embeddings:** Utilized for generating vector embeddings of text data, enabling semantic understanding and retrieval.
*   **Docker:** Used for containerizing the application, ensuring consistent environments across development and deployment.
*   **GitHub Webhooks:** To trigger the agent when code changes occur in a repository.

## Agent Logic (`agent_logic.py`)

*   **`run_agent_analysis`:** This is the core of the agent.
    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
    *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
    *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

## Vector Store (`vector_store.py`)

*   **`create_vector_store`:** This function is responsible for building the FAISS index. It takes a list of file paths, reads their content, and generates embeddings using a specified model. These embeddings are then stored in a FAISS index file (`docs.index`) and a mapping of document content to their embeddings (`doc_map.json`).
*   **`load_vector_store`:** This function loads the existing FAISS index and the document map from disk, allowing the agent to perform similarity searches.

## Broadcaster (`broadcaster.py`)

*   A utility for sending real-time status updates (logs, errors, actions) from the backend to connected clients (e.g., a frontend dashboard) via WebSockets.

## Environment Variables

*   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
*   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
*   `OPENAI_API_KEY`: Your OpenAI API key.
*   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

---

### Relevant Code Changes

```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
      *   **Build Command**: `pip install -r requirements.txt`
          *   This is usually the default and is correct.
      *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-         *   Use `10000` for the port as recommended by Render.
+         *   Use the port recommended by Render (e.g., `10000`).
  3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
  4.  **Deploy**: Trigger a manual deploy.
  5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -199,5 +199,4 @@
             if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
-
 ```
```


---

### AI-Generated Update (2025-11-16 12:01:04)

The `CONFIDENCE_THRESHOLD` environment variable can now be used to configure the minimum confidence score required for documentation updates. The default value is set to `0.2`.

## Core Technologies

*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints.
*   **LangChain:** A framework for developing applications powered by language models. It allows for the chaining of different components, such as LLMs, prompt templates, and retrievers.
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to create a searchable index of the documentation.
*   **HuggingFace Embeddings:** Utilized for generating vector embeddings of text data, enabling semantic understanding and retrieval.
*   **Docker:** Used for containerizing the application, ensuring consistent environments across development and deployment.
*   **GitHub Webhooks:** To trigger the agent when code changes occur in a repository.

## Agent Logic (`agent_logic.py`)

*   **`run_agent_analysis`:** This is the core of the agent.
    *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
    *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
    *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
    *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
    *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
    *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
    *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

## Vector Store (`vector_store.py`)

*   **`create_vector_store`:** This function is responsible for building the FAISS index. It takes a list of file paths, reads their content, and generates embeddings using a specified model. These embeddings are then stored in a FAISS index file (`docs.index`) and a mapping of document content to their embeddings (`doc_map.json`).
*   **`load_vector_store`:** This function loads the existing FAISS index and the document map from disk, allowing the agent to perform similarity searches.

## Broadcaster (`broadcaster.py`)

*   A utility for sending real-time status updates (logs, errors, actions) from the backend to connected clients (e.g., a frontend dashboard) via WebSockets.

## Environment Variables

*   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
*   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
*   `OPENAI_API_KEY`: Your OpenAI API key.
*   `CONFIDENCE_THRESHOLD`: A float value (defaulting to 0.2) used to gate documentation updates. If the confidence score of retrieved documentation is below this threshold, the update is skipped.

---

### Relevant Code Changes

```diff
diff --git a/backend/USER_GUIDE.md b/backend/USER_GUIDE.md
index 31b5e7c..629049c 100644
--- a/backend/USER_GUIDE.md
+++ b/backend/USER_GUIDE.md
@@ -86,7 +86,7 @@ The backend is a Python FastAPI application.
      *   **Build Command**: `pip install -r requirements.txt`
          *   This is usually the default and is correct.
      *   **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 10000`
-         *   Use `10000` for the port as recommended by Render.
+         *   Use the port recommended by Render (e.g., `10000`).
  3.  **Add Environment Variables**: Go to the **Environment** tab for your new service and add the same `GITHUB_SECRET_TOKEN`, `GITHUB_API_TOKEN`, and `OPENAI_API_KEY` that you have in your local `.env` file.
  4.  **Deploy**: Trigger a manual deploy.
  5.  **Update Your Webhook**: Once deployed, Render will provide a public URL (e.g., `https://your-app-name.onrender.com`). Update your GitHub webhook's **Payload URL** to point to this new URL (e.g., `https://your-app-name.onrender.com/api/webhook/github`).
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -199,5 +199,4 @@
             if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                  await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                  return
-
 ```
```


---

### AI-Generated Update (2025-11-16 12:01:30)

## Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

### Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.

### Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

### How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
    *   `OPENAI_API_KEY`: Your OpenAI API key.
    *   `CONFIDENCE_THRESHOLD`: (Optional) The minimum confidence score required to update a document. Defaults to 0.2.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes
```diff
diff --git a/backend/agent_logic.py b/backend/agent_logic.py
index 194eafb..3b3f53d 100644
--- a/backend/agent_logic.py
+++ b/backend/agent_logic.py
@@ -197,7 +197,9 @@
             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
         else:
             # --- UPDATE MODE ---
-            if confidence_score < 0.2: # Gatekeeping based on confidence
+            # Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
                 await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
                 return
 
diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
index 36e9b76..4ec729c 100644
--- a/backend/data/Knowledge_Base.md
+++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-            *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+            *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+
+---
+
+-### AI-Generated Update (2025-11-16 11:47:09)
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+---- Snippet 1 (Source: data\Knowledge_Base.md) ---
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+
+----- Snippet 2 (Source: agent_logic.py) ---
+# --- Step 9: Log the final result ---
+        if "Successfully" in result_message:
+            # On success, log the specific format you requested.
+            log_entry = (
+@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+
+def _append_to_file_sync(file_path: str, content: str):
+    """Synchronous file append operation."""
+    with open(file_path, "a", encoding="utf-8") as f:
+@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+        return
+
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+
+if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+@@ -139,44 +132,10 @@ if "Error" in pr_url:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+
+----
+### Relevant Code Changes
+```diff
+diff --git a/backend/agent_logic.py b/backend/agent_logic.py
+index 194eafb..3b3f53d 100644
+--- a/backend/agent_logic.py
++++ b/backend/agent_logic.py
+@@ -197,7 +197,9 @@
+             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+         else:
+             # --- UPDATE MODE ---
+-            if confidence_score < 0.2: # Gatekeeping based on confidence
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+-            # --- CREATE MODE ---
+-            # If no relevant docs found, or confidence is too low, create new docs
+-            if not retrieved_docs or confidence_score < 0.2:
+-                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+-                new_documentation = await creator_chain.ainvoke({
+-                    "analysis_summary": analysis_summary,
+-                    "git_diff": git_diff
+-                })
+-                # For creation, the source file is always the main knowledge base
+-                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+-            else:
+-                # --- UPDATE MODE ---
+-                # If relevant docs are found and confidence is high enough, rewrite existing docs
+-                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+-                new_documentation = await rewriter_chain.ainvoke({
+-                    "analysis_summary": analysis_summary,
+-                    "git_diff": git_diff,
+-                    "retrieved_docs": retrieved_docs
+-                })
+-                # Update the specific files that were retrieved
+-                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+-
+-        # --- Step 7: Save the new documentation ---
+-        # Append the new documentation to the relevant files
+-        # If the file doesn't exist, create it
+-        for file_path in raw_paths:
+-            if file_path: # Ensure file_path is not None
+-                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+-                # Use a synchronous append to avoid issues with concurrent file writes
+-                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+-
+-        # --- Step 8: Re-index the vector store ---
+-        # This is crucial to ensure the newly added documentation is searchable
+-        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+-        try:
+-            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+-            # In a production scenario, consider a more efficient incremental update if possible
+-            vector_store.create_vector_store()
+-            # Update the global retriever instance
+-            global retriever
+-            retriever = vector_store.get_retriever()
+-            await broadcaster("log-step", "Vector store re-indexed successfully.")
+-        except Exception as e:
+-            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+-            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+-
+-        # --- Step 9: Create a GitHub Pull Request ---
+-        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+-        pr_url = await create_github_pr_async(
+-            repo_owner,
+-            repo_name,
+-            f"docs: Update documentation based on PR #{pr_number}",
+-            "main", # base branch
+-            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+-            "Documentation updated by AI agent.",
+-            git_diff, # Include the original diff for context
+-            new_documentation # Include the generated documentation
+-        )
+-
+-        if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-        else:
+-            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+-            await broadcaster("log-success", result_message)
+-            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+-
+-    except Exception as e:
+## Doc-Ops Agent
+
+This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.
+
+### Purpose
+
+The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:
+
+*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
+*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
+*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
+*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.
+
+### Core Technologies
+
+*   **Python:** The primary programming language for the backend application.
+*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
+*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
+*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
+*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
+*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
+*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
+*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
+*   **python-dotenv:** Used to load environment variables from a `.env` file.
+*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.
+
+### How Components Work Together
+
+The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.
+
+1.  **GitHub Webhook (`/api/webhook/github`):**
+    *   This endpoint receives `POST` requests from GitHub.
+    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
+    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
+    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
+    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
+    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).
+
+2.  **Agent Logic (`agent_logic.py`):**
+    *   **`run_agent_analysis`:** This is the core of the agent.
+        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.
+
+3.  **Vector Store (`vector_store.py`):**
+    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
+        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
+        *   It loads all `.md` files from the `data/` directory.
+        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
+        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
+        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
+    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
+    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search.
+
+4.  **Live Logging (`/api/stream/logs`):**
+    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
+    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.
+
+5.  **Environment Variables:**
+    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
+    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
+    *   `OPENAI_API_KEY`: Your OpenAI API key.
+    *   `CONFIDENCE_THRESHOLD`: (Optional) The minimum confidence score required to update a document. Defaults to 0.2.
+
+In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.
+
+---
+
+### Relevant Code Changes
+```diff
+diff --git a/backend/agent_logic.py b/backend/agent_logic.py
+index 194eafb..3b3f53d 100644
+--- a/backend/agent_logic.py
++++ b/backend/agent_logic.py
+@@ -197,7 +197,9 @@
+             raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+         else:
+             # --- UPDATE MODE ---
+-            if confidence_score < 0.2: # Gatekeeping based on confidence
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+-            # --- CREATE MODE ---
+-            # If no relevant docs found, or confidence is too low, create new docs
+-            if not retrieved_docs or confidence_score < 0.2:
+-                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+-                new_documentation = await creator_chain.ainvoke({
+-                    "analysis_summary": analysis_summary,
+-                    "git_diff": git_diff
+-                })
+-                # For creation, the source file is always the main knowledge base
+-                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+-            else:
+-                # --- UPDATE MODE ---
+-                # If relevant docs are found and confidence is high enough, rewrite existing docs
+-                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+-                new_documentation = await rewriter_chain.ainvoke({
+-                    "analysis_summary": analysis_summary,
+-                    "git_diff": git_diff,
+-                    "retrieved_docs": retrieved_docs
+-                })
+-                # Update the specific files that were retrieved
+-                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+-
+-        # --- Step 7: Save the new documentation ---
+-        # Append the new documentation to the relevant files
+-        # If the file doesn't exist, create it
+-        for file_path in raw_paths:
+-            if file_path: # Ensure file_path is not None
+-                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+-                # Use a synchronous append to avoid issues with concurrent file writes
+-                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+-
+-        # --- Step 8: Re-index the vector store ---
+-        # This is crucial to ensure the newly added documentation is searchable
+-        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+-        try:
+-            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+-            # In a production scenario, consider a more efficient incremental update if possible
+-            vector_store.create_vector_store()
+-            # Update the global retriever instance
+-            global retriever
+-            retriever = vector_store.get_retriever()
+-            await broadcaster("log-step", "Vector store re-indexed successfully.")
+-        except Exception as e:
+-            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+-            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+-
+-        # --- Step 9: Create a GitHub Pull Request ---
+-        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+-        pr_url = await create_github_pr_async(
+-            repo_owner,
+-            repo_name,
+-            f"docs: Update documentation based on PR #{pr_number}",
+-            "main", # base branch
+-            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+-            "Documentation updated by AI agent.",
+-            git_diff, # Include the original diff for context
+-            new_documentation # Include the generated documentation
+-        )
+-
+-        if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-        else:
+-            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+-            await broadcaster("log-success", result_message)
+-            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+-
+-    except Exception as e:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-            await broadcaster("log-error", f"Agent failed: {e}")
+-
+-
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff,
+                    "retrieved_docs": retrieved_docs
+                })
+                # Update the specific files that were retrieved
+                raw_paths = [doc.metadata.get("source") for doc in retrieved_docs]
+
+        # --- Step 7: Save the new documentation ---
+        # Append the new documentation to the relevant files
+        # If the file doesn't exist, create it
+        for file_path in raw_paths:
+            if file_path: # Ensure file_path is not None
+                await broadcaster("log-action", f"Updating documentation in: {file_path}")
+                # Use a synchronous append to avoid issues with concurrent file writes
+                _append_to_file_sync(file_path, f"\n\n{new_documentation}")
+
+        # --- Step 8: Re-index the vector store ---
+        # This is crucial to ensure the newly added documentation is searchable
+        await broadcaster("log-step", "Re-indexing vector store with updated knowledge base...")
+        try:
+            # This will rebuild the index from scratch using the updated Knowledge_Base.md
+            # In a production scenario, consider a more efficient incremental update if possible
+            vector_store.create_vector_store()
+            # Update the global retriever instance
+            global retriever
+            retriever = vector_store.get_retriever()
+            await broadcaster("log-step", "Vector store re-indexed successfully.")
+        except Exception as e:
+            await broadcaster("log-error", f"Failed to re-index vector store: {e}")
+            logger.error(f"Failed to re-index vector store: {e}", exc_info=True)
+
+        # --- Step 9: Create a GitHub Pull Request ---
+        await broadcaster("log-action", f"Creating GitHub Pull Request for documentation update...")
+        pr_url = await create_github_pr_async(
+            repo_owner,
+            repo_name,
+            f"docs: Update documentation based on PR #{pr_number}",
+            "main", # base branch
+            f"docs-update-{pr_number}-{int(time.time())}", # head branch
+            "Documentation updated by AI agent.",
+            git_diff, # Include the original diff for context
+            new_documentation # Include the generated documentation
+        )
+
+        if "Error" in pr_url:
+                result_message = f"Failed to create PR. Reason: {pr_url}"
+                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+        else:
+            result_message = f"Successfully updated documentation and created PR: {pr_url}"
+            await broadcaster("log-success", result_message)
+            await broadcaster("log-pr-url", pr_url) # Send PR URL for frontend display
+
+    except Exception as e:
+            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+            await broadcaster("log-error", f"Agent failed: {e}")
+
+
+# --- This is the core logic change ---
+# Make the confidence threshold configurable for easier testing
+            confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", 0.2))
+            if confidence_score < confidence_threshold: # Gatekeeping based on confidence
+                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+                return
+
+            # --- CREATE MODE ---
+            # If no relevant docs found, or confidence is too low, create new docs
+            if not retrieved_docs or confidence_score < confidence_threshold:
+                await broadcaster("log-step", "No relevant docs found or confidence too low. Switching to 'Create Mode'...")
+                new_documentation = await creator_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_diff": git_diff
+                })
+                # For creation, the source file is always the main knowledge base
+                raw_paths = [os.path.join('data', 'Knowledge_Base.md')]
+            else:
+                # --- UPDATE MODE ---
+                # If relevant docs are found and confidence is high enough, rewrite existing docs
+                await broadcaster("log-step", "Relevant docs found. Switching to 'Update Mode'...")
+                new_documentation = await rewriter_chain.ainvoke({
+                    "analysis_summary": analysis_summary,
+                    "git_


---

### AI-Generated Update (2025-11-16 12:01:47)

# Doc-Ops Agent

This project provides an automated agent that monitors GitHub repositories for code changes. It analyzes these changes, retrieves relevant existing documentation, and either updates the documentation or creates new documentation if none exists. The agent then automatically creates a pull request with the generated documentation updates.

### Purpose

The primary goal of this project is to streamline and improve the documentation process for software projects. By automatically generating and updating documentation based on code changes, it aims to:

*   **Keep documentation up-to-date:** Reduce the burden on developers to manually update documentation, ensuring it accurately reflects the current codebase.
*   **Improve documentation quality:** Leverage AI to generate clear, concise, and contextually relevant documentation.
*   **Enhance discoverability:** Make it easier for team members and contributors to find and understand project information.
*   **Automate repetitive tasks:** Free up developer time by handling the creation and updating of documentation automatically.

### Core Technologies

*   **Python:** The primary programming language for the backend application.
*   **FastAPI:** A modern, fast (high-performance) web framework for building APIs with Python. It's used here to create the webhook endpoint and the live log stream.
*   **LangChain:** An open-source framework for developing applications powered by language models. It's used for orchestrating the AI's analysis, retrieval, and generation capabilities.
*   **Hugging Face Transformers:** Used for local, efficient embedding generation (`all-MiniLM-L6-v2`).
*   **FAISS:** A library for efficient similarity search and clustering of dense vectors. Used here to store and retrieve documentation embeddings.
*   **GitHub API (PyGithub):** Used to interact with GitHub, specifically for fetching code diffs and creating pull requests.
*   **Requests:** A Python HTTP library, used for making direct API calls to GitHub for diffs.
*   **Uvicorn:** An ASGI server, used to run the FastAPI application.
*   **python-dotenv:** Used to load environment variables from a `.env` file.
*   **SSE-Starlette:** A library for Server-Sent Events, used to stream logs to the frontend in real-time.

### How Components Work Together

The system is designed around a central FastAPI application that listens for GitHub events and orchestrates the AI agent's workflow.

1.  **GitHub Webhook (`/api/webhook/github`):**
    *   This endpoint receives `POST` requests from GitHub.
    *   It verifies the request's authenticity using a shared secret token (`GITHUB_SECRET_TOKEN`) and the `X-Hub-Signature-256` header.
    *   It specifically listens for `pull_request` events (when a PR is `closed` and `merged`) and `push` events.
    *   Upon receiving a relevant event, it extracts the code diff URL from the payload.
    *   It uses the `GITHUB_API_TOKEN` and the `requests` library to fetch the actual code diff content from GitHub.
    *   It then triggers the `agent_logic.run_agent_analysis` function asynchronously, passing the diff content and relevant metadata (like PR title, repo name, etc.).

2.  **Agent Logic (`agent_logic.py`):**
    *   **`run_agent_analysis`:** This is the core of the agent.
        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
        *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
        *   **Pull Request Creation:** Finally, it uses the `create_github_pr_async` function to create a new branch, stage the changes, and open a pull request on GitHub with the generated documentation.

3.  **Vector Store (`vector_store.py`):**
    *   **`create_vector_store`:** This function is responsible for building the FAISS index.
        *   It first checks if the `Knowledge_Base.md` is empty and, if so, uses a `seeder_chain` to generate an initial summary of the project's source code to populate it.
        *   It loads all `.md` files from the `data/` directory.
        *   It uses `RecursiveCharacterTextSplitter` to break down the documents into manageable chunks.
        *   It generates embeddings for these chunks using a local `HuggingFaceEmbeddings` model.
        *   It creates a FAISS index from these embeddings and saves it locally to the `faiss_index/` directory.
    *   **`load_vector_store`:** Loads an existing FAISS index from disk.
    *   **`get_retriever`:** This is the primary function used by the agent logic. It attempts to load an existing index; if none is found, it triggers `create_vector_store`. It then returns a LangChain retriever object configured for similarity search.

4.  **Live Logging (`/api/stream/logs`):**
    *   This endpoint uses Server-Sent Events (SSE) to stream log messages from the `log_queue` to any connected client (e.g., a frontend dashboard).
    *   The webhook handler and agent logic push log messages (categorized by event type like `log-step`, `log-error`, `log-action`) into this queue.

5.  **Environment Variables:**
    *   `GITHUB_SECRET_TOKEN`: Used to verify incoming GitHub webhooks.
    *   `GITHUB_API_TOKEN`: Used by the agent to authenticate with the GitHub API for fetching diffs and creating pull requests.
    *   `OPENAI_API_KEY`: Your OpenAI API key.
    *   `CONFIDENCE_THRESHOLD`: (Optional) The minimum confidence score required to update a document. Defaults to 0.2.

In essence, the system acts as an automated documentation assistant. It watches for code changes, uses AI to understand them and interact with existing knowledge, updates that knowledge, and then proposes the documentation changes back to the project via a pull request.

---

### Relevant Code Changes
```diff
diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
index 36e9b76..4ec729c 100644
--- a/backend/data/Knowledge_Base.md
+++ b/backend/data/Knowledge_Base.md
@@ -40,7 +40,7 @@
     *   **`run_agent_analysis`:** This is the core of the agent.
          *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
          *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
          *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
          *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
          *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+ 
+ ---
+ 
+-### AI-Generated Update (2025-11-16 11:47:09)
+-
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
+-                await broadcaster("log-error", f"Failed to create PR: {pr_url}")
+-@@ -139,44 +132,10 @@ if "Error" in pr_url:
+-            logger.error(f"Agent failed for PR #{pr_number} ({repo_name}) with error: {e}", exc_info=True)
+-        
+----
+-### Relevant Code Changes
+-```diff
+-diff --git a/backend/data/Knowledge_Base.md b/backend/data/Knowledge_Base.md
+-index 36e9b76..4ec729c 100644
+---- a/backend/data/Knowledge_Base.md
+-+++ b/backend/data/Knowledge_Base.md
+-@@ -40,7 +40,7 @@
+-     *   **`run_agent_analysis`:** This is the core of the agent.
+-         *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-         *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a threshold, it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-+++        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-         *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-         *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-         *   **Vector Store Re-indexing:** After updating the knowledge base, the `vector_store.create_vector_store()` function is called to rebuild the FAISS index, incorporating the new information. This makes the agent immediately "smarter" for subsequent runs.
+-@@ -68,27 +68,18 @@ In essence, the system acts as an automated documentation assistant. It watches
+- 
+- ---
+- 
+--### AI-Generated Update (2025-11-16 11:47:09)
+--
+--2.  **Agent Logic (`agent_logic.py`):**
+--    *   **`run_agent_analysis`:** This is the core of the agent.
+--        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+--        *   If it's a functional change, it uses a `retriever` (powered by FAISS and HuggingFace embeddings) to search a vector store of existing documentation for relevant snippets based on the analysis summary.
+--        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+--        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+--        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 1 (Source: data\Knowledge_Base.md) ---
+-2.  **Agent Logic (`agent_logic.py`):**
+-    *   **`run_agent_analysis`:** This is the core of the agent.
+-        *   It first uses an `analyzer_chain` (powered by LangChain) to analyze the provided `git_diff`, determining if it's a functional change and summarizing it.
+-@@ -86,118 +5,3162 @@ In essence, the system acts as an automated documentation assistant. It watches
+-        *   **Update Mode:** If relevant documentation is found and the confidence score is above a configurable `CONFIDENCE_THRESHOLD` (defaults to 0.2), it uses a `rewriter_chain` to generate updated documentation based on the analysis and the retrieved snippets.
+-        *   **Create Mode:** If no relevant documentation is found, it uses a `creator_chain` to generate entirely new documentation based on the analysis and the diff.
+-        *   **Knowledge Base Update:** The newly generated documentation is appended to a central `Knowledge_Base.md` file.
+-
+----- Snippet 2 (Source: agent_logic.py) ---
+-# --- Step 9: Log the final result ---
+-        if "Successfully" in result_message:
+-            # On success, log the specific format you requested.
+-            log_entry = (
+-@@ -108,7 +99,8 @@ await broadcaster("log-step", f"Found {len(retrieved_docs)} relevant doc snippets. Confidence: {confidence_percent}")
+-                await broadcaster("log-skip", f"Confidence ({confidence_percent}) is below threshold. Skipping doc update.")
+-                return
+-
+----- Snippet 3 (Source: agent_logic.py) ---
+---
+-
+-def _append_to_file_sync(file_path: str, content: str):
+-    """Synchronous file append operation."""
+-    with open(file_path, "a", encoding="utf-8") as f:
+-@@ -124,7 +116,8 @@ async def run_agent_analysis(logger, broadcaster, git_diff: str, pr_title: str,
+-        await broadcaster("log-error", "Error: Agent AI components are not ready.")
+-        return
+-
+----- Snippet 4 (Source: agent_logic.py) ---
+---
+-
+-if "Error" in pr_url:
+-                result_message = f"Failed to create PR. Reason: {pr_url}"
